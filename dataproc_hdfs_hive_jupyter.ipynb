{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicating a Single-Node Cloudera Environment with Dataproc\n",
    "This notebook provides a definitive, step-by-step guide to creating a single-node Hadoop environment on Google Cloud Dataproc. It is designed to be run cell-by-cell, first locally to create the infrastructure, and then within the cluster's Jupyter environment to perform data operations correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configuration\n",
    "First, we define the configuration for our Dataproc cluster. **Make sure to replace the placeholder values** with your specific Google Cloud project details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Fill in these values before running!\n",
    "PROJECT_ID = \"your-gcp-project-id\"  # e.g., my-gcp-project\n",
    "REGION = \"your-gcp-region\"      # e.g., us-central1\n",
    "CLUSTER_NAME = \"my-single-node-cluster\"\n",
    "# A unique GCS bucket name. Using the project ID as a prefix is a good practice.\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-dataproc-bucket\"\n",
    "# A BigQuery dataset to act as a persistent Iceberg metastore.\n",
    "BQ_DATASET = \"my_iceberg_metastore\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2a: Create GCS Bucket and a Fully Configured Dataproc Cluster\n",
    "This is the most important step. We add a `--properties` flag to the cluster creation command. This configures the default Spark session for the entire cluster, so we don't need to manage the session lifecycle within the notebook. This is the standard and correct way to set these configurations.\n",
    "\n",
    "The properties we set are:\n",
    "- `spark:spark.sql.warehouse.dir`: Sets the Hive warehouse to an HDFS path, fixing the primary Hive error.\n",
    "- `spark:spark.sql.catalog.biglake`: Pre-configures the BigLake catalog for Iceberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the bucket exists and create it if it does not\n",
    "!gcloud storage buckets describe gs://{BUCKET_NAME} || gcloud storage buckets create gs://{BUCKET_NAME} --location={REGION}\n",
    "\n",
    "# Define the properties for the Spark session\n",
    "PROPERTIES = f\"spark:spark.sql.warehouse.dir=hdfs:///user/hive/warehouse,spark:spark.sql.catalog.biglake=org.apache.iceberg.gcp.bigquery.BigQueryCatalog,spark:spark.sql.catalog.biglake.project={PROJECT_ID},spark:spark.sql.catalog.biglake.location={REGION},spark:spark.sql.catalog.biglake.gcs_location=gs://{BUCKET_NAME}/iceberg_warehouse\"\n",
    "\n",
    "# Create the Dataproc cluster with the session properties\n",
    "!gcloud dataproc clusters create {CLUSTER_NAME} \\\n",
    "    --region {REGION} \\\n",
    "    --single-node \\\n",
    "    --image-version 2.2-debian12 \\\n",
    "    --optional-components=JUPYTER,ICEBERG \\\n",
    "    --enable-component-gateway \\\n",
    "    --bucket {BUCKET_NAME} \\\n",
    "    --properties=\"{PROPERTIES}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2b: Update an Existing Cluster (Optional)\n",
    "If you already have a cluster running and want to apply or change the configuration without recreating it, you can use the `gcloud dataproc clusters update` command. Note that some properties may require a cluster restart to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the properties for the Spark session\n",
    "PROPERTIES = f\"spark:spark.sql.warehouse.dir=hdfs:///user/hive/warehouse,spark:spark.sql.catalog.biglake=org.apache.iceberg.gcp.bigquery.BigQueryCatalog,spark:spark.sql.catalog.biglake.project={PROJECT_ID},spark:spark.sql.catalog.biglake.location={REGION},spark:spark.sql.catalog.biglake.gcs_location=gs://{BUCKET_NAME}/iceberg_warehouse\"\n",
    "\n",
    "# Update the cluster with the new properties\n",
    "!gcloud dataproc clusters update {CLUSTER_NAME} \\\n",
    "    --region {REGION} \\\n",
    "    --update-properties=\"{PROPERTIES}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Accessing the Jupyter Notebook\n",
    "For interactive development, use the Jupyter environment running on the cluster.\n",
    "\n",
    "**How to Access Jupyter:**\n",
    "1. Navigate to the **Dataproc** section in the Google Cloud Console.\n",
    "2. Click on your cluster's name (`my-single-node-cluster`).\n",
    "3. Go to the **Web Interfaces** tab.\n",
    "4. Click the **Jupyter** link. This will open the Jupyter environment in a new browser tab.\n",
    "\n",
    "---\n",
    "### **--- The following cells (Steps 4-8) are intended to be run inside the Dataproc Jupyter Environment ---**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Get Spark Session and Create DataFrame\n",
    "We no longer need to stop or configure the session. We simply get the default session that was already configured for us when the cluster was created. This is the simplest and most reliable approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Get the existing, pre-configured Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Create a sample DataFrame that we will reuse across multiple steps.\n",
    "df = spark.createDataFrame([('Alice', 25), ('Bob', 30), ('Charlie', 35)], ['name', 'age'])\n",
    "\n",
    "print(\"Spark session is ready and sample DataFrame 'df' has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Write to HDFS\n",
    "Using the DataFrame created in the previous step, we will now write it to a new directory in HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory in HDFS\n",
    "!hdfs dfs -mkdir -p /user/my_data\n",
    "\n",
    "# Write the DataFrame to that directory as a Parquet file\n",
    "df.write.mode('overwrite').parquet('/user/my_data/people')\n",
    "\n",
    "# Verify the data was written by listing the contents of the directory\n",
    "!hdfs dfs -ls /user/my_data/people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Create a Hive Table\n",
    "Because the Spark session is now correctly configured, creating a Hive database and table will work without any errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a database\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS my_db\")\n",
    "spark.sql(\"USE my_db\")\n",
    "\n",
    "# Create an external Hive table pointing to the data in HDFS\n",
    "spark.sql(\"CREATE EXTERNAL TABLE IF NOT EXISTS people (name STRING, age INT) STORED AS PARQUET LOCATION '/user/my_data/people'\")\n",
    "\n",
    "# Query the table\n",
    "spark.sql(\"SELECT * FROM my_db.people\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Create an Iceberg Table (using Hive Metastore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `spark_catalog` is the default catalog and is already configured to use Hive.\n",
    "iceberg_hive_table = \"spark_catalog.my_db.people_iceberg\"\n",
    "\n",
    "# Save the DataFrame as an Iceberg table\n",
    "df.write.mode(\"overwrite\").format(\"iceberg\").save(iceberg_hive_table)\n",
    "\n",
    "# Query the table\n",
    "spark.sql(f\"SELECT * FROM {iceberg_hive_table}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Create an Iceberg Table (using BigLake Metastore)\n",
    "Because the `biglake` catalog was configured at the cluster level, we can use it directly without any further configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the BigQuery dataset that will act as the metastore\n",
    "!bq --location={REGION} mk --dataset {PROJECT_ID}:{BQ_DATASET}\n",
    "\n",
    "# Define the BigLake table name\n",
    "biglake_table = f\"biglake.{BQ_DATASET}.people_biglake\"\n",
    "\n",
    "# Save the DataFrame as an Iceberg table using the BigLake catalog\n",
    "df.write.mode(\"overwrite\").format(\"iceberg\").save(biglake_table)\n",
    "\n",
    "# Query the table from Spark\n",
    "spark.sql(f\"SELECT * FROM {biglake_table}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Clean Up\n",
    "To avoid incurring ongoing charges, delete the cluster after you are finished. Run the following command in your local terminal or a new notebook cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud dataproc clusters delete {CLUSTER_NAME} --region {REGION}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}