{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataproc with BigLake Metastore\n",
    "\n",
    "A guide to setting up a single-node Dataproc cluster with a BigLake Metastore.\n",
    "\n",
    "**Note:** Steps 1-3 are for provisioning the cluster from your local environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configure Cluster Settings\n",
    "\n",
    "Set the configuration for your Dataproc cluster. **Replace placeholders** with your Google Cloud project details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG: Replace these values with your project details.\n",
    "PROJECT_ID = \"my-project-id\"  # Your Google Cloud project ID\n",
    "REGION = \"us-central1\"      # The region for the cluster\n",
    "CLUSTER_NAME = \"my-single-node-cluster\"  # A name for your Dataproc cluster\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-dataproc-bucket\"  # A unique GCS bucket name\n",
    "\n",
    "# Hive settings\n",
    "HIVE_DB = \"my_hive_db\"\n",
    "HIVE_TABLE = f\"{HIVE_DB}.people_hive\"\n",
    "HIVE_GCS_TABLE = f\"{HIVE_DB}.people_hive_gcs\"\n",
    "\n",
    "# Iceberg on Hive settings\n",
    "ICEBERG_HIVE_CATALOG = \"iceberg_on_hive\"\n",
    "ICEBERG_HIVE_DB = \"my_iceberg_db\"\n",
    "ICEBERG_HIVE_TABLE = f\"{ICEBERG_HIVE_CATALOG}.{ICEBERG_HIVE_DB}.people_iceberg\"\n",
    "ICEBERG_HIVE_FROM_BQ = f\"{ICEBERG_HIVE_CATALOG}.{ICEBERG_HIVE_DB}.people_filtered_bq\"\n",
    "ICEBERG_HIVE_FROM_SPARK = f\"{ICEBERG_HIVE_CATALOG}.{ICEBERG_HIVE_DB}.people_filtered_spark\"\n",
    "\n",
    "# Iceberg on BigLake Metastore settings\n",
    "BIGLAKE_DATASET = \"my_iceberg_metastore\"\n",
    "BIGLAKE_TABLE = f\"{BIGLAKE_DATASET}.people_biglake\"\n",
    "ICEBERG_BIGLAKE_CATALOG = \"iceberg_on_bq\"\n",
    "ICEBERG_BIGLAKE_TABLE = f\"{ICEBERG_BIGLAKE_CATALOG}.{BIGLAKE_TABLE}\"\n",
    "ICEBERG_BIGLAKE_FROM_SPARK = f\"{ICEBERG_BIGLAKE_CATALOG}.{BIGLAKE_DATASET}.people_filtered_spark\"\n",
    "\n",
    "# # Trino on GCE settings\n",
    "# GCE_INSTANCE_TRINO = \"trino-biglake-vm\"\n",
    "# GCE_ZONE_TRINO = \"us-central1-a\"\n",
    "GCE_INSTANCE_SPARK = \"spark-biglake-vm\"\n",
    "GCE_ZONE_SPARK = \"us-central1-a\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create and Upload Initialization Script\n",
    "\n",
    "This script downloads Iceberg and BigQuery JARs and places them in Spark's classpath. It's uploaded to a GCS bucket for use during cluster creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script downloads required JARs for Iceberg on BigLake Metastore integration.\n",
    "init_script_content = \"\"\"#!/bin/bash\n",
    "# install-jars.sh\n",
    "set -e -x\n",
    "\n",
    "# URLs for the JAR files\n",
    "ICEBERG_RUNTIME_URL=\"https://storage-download.googleapis.com/maven-central/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.6.1/iceberg-spark-runtime-3.5_2.12-1.6.1.jar\"\n",
    "BQ_CATALOG_JAR_GCS=\"gs://spark-lib/bigquery/iceberg-bigquery-catalog-1.6.1-1.0.1-beta.jar\"\n",
    "\n",
    "# Download JARs to Spark's classpath\n",
    "wget -P /usr/lib/spark/jars/ \"$ICEBERG_RUNTIME_URL\"\n",
    "gsutil cp \"$BQ_CATALOG_JAR_GCS\" /usr/lib/spark/jars/\n",
    "\"\"\"\n",
    "\n",
    "# Define local and GCS paths\n",
    "local_script_path = \"install-jars.sh\"\n",
    "gcs_script_path = f\"gs://{BUCKET_NAME}/scripts/{local_script_path}\"\n",
    "\n",
    "# Write the script to a local file\n",
    "with open(local_script_path, \"w\") as f:\n",
    "    f.write(init_script_content)\n",
    "\n",
    "print(f\"Initialization script created at: {local_script_path}\")\n",
    "\n",
    "# Upload the script to GCS\n",
    "!gsutil cp {local_script_path} {gcs_script_path}\n",
    "\n",
    "print(f\"Successfully uploaded script to: {gcs_script_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Dataproc Cluster with Iceberg Catalog\n",
    "\n",
    "This command creates a Dataproc cluster with the necessary Spark properties for Iceberg. Setting these at the cluster level ensures the Spark session is pre-configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define properties for the Dataproc cluster\n",
    "properties_list = [\n",
    "    \"spark:spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "    f\"spark:spark.sql.catalog.{ICEBERG_HIVE_CATALOG}=org.apache.iceberg.spark.SparkCatalog\",\n",
    "    f\"spark:spark.sql.catalog.{ICEBERG_HIVE_CATALOG}.type=hive\",\n",
    "    f\"spark:spark.sql.catalog.{ICEBERG_HIVE_CATALOG}.warehouse=hdfs:///user/iceberg_on_hive\",\n",
    "    f\"spark:spark.sql.catalog.{ICEBERG_BIGLAKE_CATALOG}=org.apache.iceberg.spark.SparkCatalog\",\n",
    "    f\"spark:spark.sql.catalog.{ICEBERG_BIGLAKE_CATALOG}.catalog-impl=org.apache.iceberg.gcp.bigquery.BigQueryMetastoreCatalog\",\n",
    "    f\"spark:spark.sql.catalog.{ICEBERG_BIGLAKE_CATALOG}.gcp_project={PROJECT_ID}\",\n",
    "    f\"spark:spark.sql.catalog.{ICEBERG_BIGLAKE_CATALOG}.location={REGION}\",\n",
    "    f\"spark:spark.sql.catalog.{ICEBERG_BIGLAKE_CATALOG}.warehouse=gs://{BUCKET_NAME}/{ICEBERG_BIGLAKE_CATALOG}\"\n",
    "]\n",
    "PROPERTIES = \",\".join(properties_list)\n",
    "\n",
    "# Create the Dataproc cluster\n",
    "!gcloud dataproc clusters create {CLUSTER_NAME} \\\n",
    "    --project {PROJECT_ID} \\\n",
    "    --region {REGION} \\\n",
    "    --single-node \\\n",
    "    --image-version 2.2-debian12 \\\n",
    "    --optional-components=JUPYTER \\\n",
    "    --enable-component-gateway \\\n",
    "    --bucket {BUCKET_NAME} \\\n",
    "    --initialization-actions={gcs_script_path} \\\n",
    "    --properties=\"{PROPERTIES}\"\n",
    "\n",
    "print(f\"Cluster '{CLUSTER_NAME}' creation initiated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Access Jupyter and Get Spark Session\n",
    "\n",
    "Once the cluster is running, access the Jupyter environment. The Spark session is pre-configured, so no extra setup is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pre-configured Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(\"Spark session is active and ready to use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create a Sample DataFrame and Write to HDFS\n",
    "\n",
    "Create a sample DataFrame and write it to HDFS as a Parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample DataFrame\n",
    "df = spark.createDataFrame(\n",
    "    [('Alice', 25), ('Bob', 30), ('Charlie', 35)], ['name', 'age'])\n",
    "\n",
    "# Write the DataFrame to HDFS\n",
    "df.write.mode('overwrite').parquet('/user/my_data/people')\n",
    "\n",
    "# Verify the file was created in HDFS\n",
    "!hdfs dfs -ls /user/my_data/people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 1: Create a Hive Table from HDFS\n",
    "\n",
    "Create a Hive table from the data in HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Hive database\n",
    "spark.sql(\n",
    "    f\"CREATE DATABASE IF NOT EXISTS {HIVE_DB} LOCATION 'hdfs:///user/hive_db'\")\n",
    "\n",
    "# Drop the table if it already exists\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {HIVE_TABLE}\")\n",
    "\n",
    "# Create an external Hive table pointing to the HDFS data\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE EXTERNAL TABLE IF NOT EXISTS {HIVE_TABLE} (\n",
    "        name STRING,\n",
    "        age BIGINT\n",
    "    )\n",
    "    STORED AS PARQUET\n",
    "    LOCATION '/user/my_data/people'\n",
    "\"\"\")\n",
    "\n",
    "print(\"--- Standard Hive Table ---\")\n",
    "# Query the Hive table\n",
    "spark.sql(f\"SELECT * FROM {HIVE_TABLE}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 2: Create an Iceberg Table from HDFS\n",
    "\n",
    "Create an Iceberg table using the Hive metastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verifying the Iceberg catalog...\")\n",
    "# Show databases in the Hive Iceberg catalog\n",
    "spark.sql(f\"SHOW DATABASES IN {ICEBERG_HIVE_CATALOG}\").show()\n",
    "\n",
    "# Create a database in the Hive Iceberg catalog\n",
    "spark.sql(\n",
    "    f\"CREATE DATABASE IF NOT EXISTS {ICEBERG_HIVE_CATALOG}.{ICEBERG_HIVE_DB}\")\n",
    "\n",
    "# Write the DataFrame to an Iceberg table\n",
    "spark.sql(f\"SELECT * FROM {HIVE_TABLE}\") \\\n",
    "    .write \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(ICEBERG_HIVE_TABLE)\n",
    "\n",
    "print(\"--- Iceberg Table on Hive Metastore ---\")\n",
    "# Query the Iceberg table\n",
    "spark.sql(f\"SELECT * FROM {ICEBERG_HIVE_TABLE}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 3: Create a Hive (or Iceberg) Table from GCS\n",
    "\n",
    "Copy the Parquet file to GCS and create a Hive table from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define GCS path and create the GCS directory if needed\n",
    "GCS_PATH = f\"gs://{BUCKET_NAME}/hive_on_gcs/data\"\n",
    "!touch _CF && gsutil cp -r _CF {GCS_PATH}/ && rm _CF\n",
    "\n",
    "# Copy the HDFS data to GCS\n",
    "!hdfs dfs -cp /user/my_data/people {GCS_PATH}/{HIVE_GCS_TABLE}\n",
    "\n",
    "# Drop the table if it already exists\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {HIVE_GCS_TABLE}\")\n",
    "\n",
    "# Create an external Hive table pointing to the GCS data\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE EXTERNAL TABLE IF NOT EXISTS {HIVE_GCS_TABLE} (\n",
    "        name STRING,\n",
    "        age BIGINT\n",
    "    )\n",
    "    STORED AS PARQUET\n",
    "    LOCATION '{GCS_PATH}/{HIVE_GCS_TABLE}'\n",
    "\"\"\")\n",
    "\n",
    "print(\"--- Hive Table on GCS ---\")\n",
    "# Query the Hive table\n",
    "spark.sql(f\"SELECT * FROM {HIVE_GCS_TABLE}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 4: Create an Iceberg Table with BigLake Metastore\n",
    "\n",
    "Create an Iceberg table using the BigLake metastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verifying the BigQuery catalog...\")\n",
    "# Show databases in the BigQuery Iceberg catalog\n",
    "spark.sql(f\"SHOW DATABASES IN {ICEBERG_BIGLAKE_CATALOG}\").show()\n",
    "\n",
    "# Create the BigQuery dataset to act as the metastore\n",
    "!bq mk --connection --location={REGION} --project_id={PROJECT_ID} --connection_type=CLOUD_RESOURCE default-{REGION}\n",
    "!bq --location={REGION} mk --dataset {PROJECT_ID}:{BIGLAKE_DATASET}\n",
    "\n",
    "# Drop the table if it already exists\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {ICEBERG_BIGLAKE_TABLE}\")\n",
    "\n",
    "# Create the Iceberg table in the BigLake metastore\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS\n",
    "  {ICEBERG_BIGLAKE_TABLE} ( name string,\n",
    "    age int )\n",
    "USING\n",
    "  ICEBERG TBLPROPERTIES ('bq_connection'='projects/{PROJECT_ID}/locations/{REGION}/connections/default-{REGION}');\n",
    "\"\"\")\n",
    "\n",
    "# Save the DataFrame to the BigLake metastore\n",
    "spark.sql(f\"SELECT * FROM {ICEBERG_HIVE_TABLE}\") \\\n",
    "    .write \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(ICEBERG_BIGLAKE_TABLE)\n",
    "\n",
    "# Query the Iceberg table from Spark\n",
    "# spark.sql(f\"SELECT * FROM {ICEBERG_BIGLAKE_TABLE}\").show()\n",
    "\n",
    "sql_query = f\"SELECT * FROM {BIGLAKE_TABLE}\"\n",
    "df = spark.read \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"viewsEnabled\", \"true\") \\\n",
    "    .option(\"query\", sql_query) \\\n",
    "    .option(\"materializationDataset\", BIGLAKE_DATASET) \\\n",
    "    .load()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 5: Push Down Computation to BigQuery\n",
    "\n",
    "Use the BigQuery Connector to execute a SQL query directly in BigQuery. Only the results are returned to Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This query is executed directly in BigQuery\n",
    "bq_sql_query = f\"\"\"\n",
    "SELECT\n",
    "    name,\n",
    "    age\n",
    "FROM\n",
    "    {BIGLAKE_TABLE}\n",
    "WHERE\n",
    "    age > 28\n",
    "\"\"\"\n",
    "\n",
    "# Use the 'bigquery' format to send the query to BigQuery\n",
    "filtered_df = spark.read \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"viewsEnabled\", \"true\") \\\n",
    "    .option(\"query\", bq_sql_query) \\\n",
    "    .option(\"materializationDataset\", BIGLAKE_DATASET) \\\n",
    "    .load()\n",
    "\n",
    "print(f\"--- Data returned from BigQuery ---\")\n",
    "filtered_df.show()\n",
    "\n",
    "# Save the filtered results to the Hive metastore\n",
    "filtered_df.write.format(\"iceberg\").mode(\n",
    "    \"overwrite\").saveAsTable(ICEBERG_HIVE_FROM_BQ)\n",
    "\n",
    "print(\"--- Iceberg Table on Hive Metastore ---\")\n",
    "spark.sql(f\"SELECT * FROM {ICEBERG_HIVE_FROM_BQ}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 6: Push Down Computation with Serverless Spark\n",
    "\n",
    "Use Serverless Spark to execute a query. The results are written to the Hive metastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python script for the Spark job\n",
    "pyspark_job_content = f\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# These values are injected by the gcloud command\n",
    "ICEBERG_BIGLAKE_TABLE = \"{ICEBERG_BIGLAKE_TABLE}\"\n",
    "ICEBERG_BIGLAKE_FROM_SPARK = f\"{ICEBERG_BIGLAKE_FROM_SPARK}\"\n",
    "\n",
    "def main():\n",
    "    spark = SparkSession.builder \\\\\n",
    "        .appName(\"Dataproc Serverless Spark Filter\") \\\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # This is a Spark SQL query, not a BigQuery query\n",
    "    filter_query = f'''\n",
    "    SELECT\n",
    "        name,\n",
    "        age\n",
    "    FROM\n",
    "        {{ICEBERG_BIGLAKE_TABLE}}\n",
    "    WHERE\n",
    "        age > 28\n",
    "    '''\n",
    "\n",
    "    print(f\"--- Running Spark SQL query: {{filter_query}} ---\")\n",
    "\n",
    "    filtered_df = spark.sql(filter_query)\n",
    "\n",
    "    print(\"--- Filtered data computed by Spark ---\")\n",
    "    filtered_df.show()\n",
    "\n",
    "    # Save results to a new Iceberg table\n",
    "    print(f\"--- Saving results to: {{ICEBERG_BIGLAKE_FROM_SPARK}} ---\")\n",
    "    filtered_df.write \\\\\n",
    "        .format(\"iceberg\") \\\\\n",
    "        .mode(\"overwrite\") \\\\\n",
    "        .saveAsTable(ICEBERG_BIGLAKE_FROM_SPARK)\n",
    "\n",
    "    print(\"Job completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\"\n",
    "\n",
    "# Define local and GCS paths for the job script\n",
    "local_job_path = \"filter_job_spark_sql.py\"\n",
    "gcs_job_path = f\"gs://{BUCKET_NAME}/scripts/{local_job_path}\"\n",
    "\n",
    "# Write and upload the script\n",
    "with open(local_job_path, \"w\") as f:\n",
    "    f.write(pyspark_job_content)\n",
    "\n",
    "!gsutil cp {local_job_path} {gcs_job_path}\n",
    "\n",
    "print(f\"Successfully uploaded Spark job script to: {gcs_job_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define properties for the serverless Dataproc job\n",
    "properties_list = [\n",
    "    \"spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "    f\"spark.sql.catalog.{ICEBERG_BIGLAKE_CATALOG}=org.apache.iceberg.spark.SparkCatalog\",\n",
    "    f\"spark.sql.catalog.{ICEBERG_BIGLAKE_CATALOG}.catalog-impl=org.apache.iceberg.gcp.bigquery.BigQueryMetastoreCatalog\",\n",
    "    f\"spark.sql.catalog.{ICEBERG_BIGLAKE_CATALOG}.gcp_project={PROJECT_ID}\",\n",
    "    f\"spark.sql.catalog.{ICEBERG_BIGLAKE_CATALOG}.location={REGION}\",\n",
    "    f\"spark.sql.catalog.{ICEBERG_BIGLAKE_CATALOG}.warehouse=gs://{BUCKET_NAME}/{ICEBERG_BIGLAKE_CATALOG}\"\n",
    "]\n",
    "PROPERTIES = \",\".join(properties_list)\n",
    "\n",
    "# Submit the PySpark job to a serverless Dataproc cluster\n",
    "!gcloud dataproc batches submit pyspark {gcs_job_path} \\\n",
    "    --project={PROJECT_ID} \\\n",
    "    --region={REGION} \\\n",
    "    --batch=\"serverless-spark-engine-job\" \\\n",
    "    --version=\"2.2\" \\\n",
    "    --subnet=\"default\" \\\n",
    "    --jars=\"gs://spark-lib/bigquery/iceberg-bigquery-catalog-1.6.1-1.0.1-beta.jar,https://storage-download.googleapis.com/maven-central/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.6.1/iceberg-spark-runtime-3.5_2.12-1.6.1.jar\" \\\n",
    "    --properties=\"{PROPERTIES}\"\n",
    "\n",
    "print(\"Serverless batch job submitted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the table created by the serverless Spark job\n",
    "print(f\"--- Data returned from Spark query ---\")\n",
    "spark.sql(f\"SELECT * FROM {ICEBERG_BIGLAKE_FROM_SPARK}\").show()\n",
    "\n",
    "# Create a new Iceberg table from the results\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {ICEBERG_HIVE_FROM_SPARK}\n",
    "    USING iceberg\n",
    "    AS\n",
    "    SELECT * FROM {ICEBERG_BIGLAKE_FROM_SPARK}\n",
    "\"\"\")\n",
    "\n",
    "print(\"--- Iceberg Table on Hive Metastore ---\")\n",
    "spark.sql(f\"SELECT * FROM {ICEBERG_HIVE_FROM_SPARK}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [NOT WORKING] Scenario 7: Trino on GCE with BigLake Metastore\n",
    "#### Notes: Currently there's limited/no header support on the Iceberg connector for Trino 476 *header.x-goog-user-project*, which is needed to connect to *v1beta/restcatalog*.\n",
    "\n",
    "This scenario demonstrates how to set up a Trino server on a Google Compute Engine (GCE) virtual machine, install Trino using Docker, and configure it to access the BigLake Metastore via the Iceberg REST API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a GCE VM\n",
    "\n",
    "First, create a GCE instance with the necessary scopes to interact with BigQuery and Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a GCE instance\n",
    "# startup_script = \"\"\"#!/bin/bash\n",
    "# sudo apt-get update\n",
    "# sudo apt-get install -y apt-transport-https ca-certificates curl gnupg lsb-release\n",
    "# curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n",
    "# echo \\\n",
    "#   \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian \\\n",
    "#   $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\n",
    "# sudo apt-get update\n",
    "# sudo apt-get install -y docker-ce docker-ce-cli containerd.io\n",
    "# sudo systemctl start docker\n",
    "# sudo systemctl enable docker\n",
    "# \"\"\"\n",
    "\n",
    "# # Write the script to a local file in your notebook environment\n",
    "# with open(\"startup.sh\", \"w\") as f:\n",
    "#     f.write(startup_script)\n",
    "\n",
    "# print(\"Startup script saved to startup.sh\")\n",
    "\n",
    "# # Create the GCE instance using the script file\n",
    "# # This is the corrected command\n",
    "# !gcloud compute instances create {GCE_INSTANCE_TRINO} \\\n",
    "#     --project={PROJECT_ID} \\\n",
    "#     --zone={GCE_ZONE_TRINO} \\\n",
    "#     --machine-type=e2-medium \\\n",
    "#     --image-family=debian-11 \\\n",
    "#     --image-project=debian-cloud \\\n",
    "#     --scopes=https://www.googleapis.com/auth/cloud-platform \\\n",
    "#     --tags=http-server,https-server \\\n",
    "#     --metadata-from-file=startup-script=startup.sh\n",
    "\n",
    "# print(f\"GCE instance '{GCE_INSTANCE_TRINO}' creation initiated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and Configure Trino with Docker\n",
    "\n",
    "Once the VM is running, SSH into it and install Docker. Then, configure Trino to use the BigLake Metastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Define the final shell script that manually fetches a token.\n",
    "# trino_script = f\"\"\"#!/bin/bash\n",
    "# set -e # Exit immediately if a command exits with a non-zero status.\n",
    "\n",
    "# echo \"--- SCRIPT START ---\"\n",
    "\n",
    "# # Install jq to parse JSON\n",
    "# sudo apt-get update -y && sudo apt-get install -y jq\n",
    "\n",
    "# # Manually fetch an OAuth2 access token\n",
    "# echo \"Fetching access token...\"\n",
    "# ACCESS_TOKEN=$(curl -s \"http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/token\" -H \"Metadata-Flavor: Google\" | jq -r .access_token)\n",
    "# if [ -z \"$ACCESS_TOKEN\" ]; then\n",
    "#     echo \"FATAL: Failed to fetch access token.\"\n",
    "#     exit 1\n",
    "# fi\n",
    "# echo \"Access token fetched successfully.\"\n",
    "\n",
    "# # Create Trino config directories\n",
    "# mkdir -p ~/trino/etc/catalog\n",
    "\n",
    "# # Create the Iceberg catalog file with the CORRECT FILENAME and ALL PROPERTIES\n",
    "# echo \"Creating Trino config...\"\n",
    "# cat <<EOF > ~/trino/etc/catalog/{ICEBERG_BIGLAKE_CATALOG}.properties\n",
    "# connector.name=iceberg\n",
    "# iceberg.catalog.type=rest\n",
    "# iceberg.rest-catalog.uri=https://biglake.googleapis.com/iceberg/v1beta/restcatalog\n",
    "# iceberg.rest-catalog.warehouse=gs://{BUCKET_NAME}/{ICEBERG_BIGLAKE_CATALOG}\n",
    "# iceberg.rest-catalog.security=GCS_OAUTH\n",
    "# iceberg.rest-catalog.gcs.oauth.scope=https://www.googleapis.com/auth/cloud-platform\n",
    "# EOF\n",
    "\n",
    "# # Create the other Trino config files\n",
    "# cat <<EOF > ~/trino/etc/jvm.config\n",
    "# -server\n",
    "# -Xmx1G\n",
    "# -XX:+UseG1GC\n",
    "# -XX:G1HeapRegionSize=32M\n",
    "# -XX:+UseGCOverheadLimit\n",
    "# -XX:+ExplicitGCInvokesConcurrent\n",
    "# -XX:+HeapDumpOnOutOfMemoryError\n",
    "# -XX:+ExitOnOutOfMemoryError\n",
    "# EOF\n",
    "# cat <<EOF > ~/trino/etc/node.properties\n",
    "# node.environment=production\n",
    "# node.data-dir=/data/trino\n",
    "# EOF\n",
    "# cat <<EOF > ~/trino/etc/config.properties\n",
    "# coordinator=true\n",
    "# node-scheduler.include-coordinator=true\n",
    "# http-server.http.port=8080\n",
    "# discovery.uri=http://localhost:8080\n",
    "# EOF\n",
    "\n",
    "# # Stop and remove any existing Trino container\n",
    "# sudo docker stop trino || true\n",
    "# sudo docker rm trino || true\n",
    "\n",
    "# # Pull and run the Trino container\n",
    "# echo \"Starting Trino container...\"\n",
    "# sudo docker run -d --name trino \\\n",
    "#     -p 8080:8080 \\\n",
    "#     -v ~/trino/etc:/etc/trino \\\n",
    "#     trinodb/trino:476\n",
    "\n",
    "# echo \"--- SCRIPT FINISHED ---\"\n",
    "# echo \"Trino should now be starting successfully. Please check 'docker logs trino'.\"\n",
    "# \"\"\"\n",
    "\n",
    "# # Step 2: Write the script to a local file.\n",
    "# with open(\"setup_trino.sh\", \"w\") as f:\n",
    "#     f.write(trino_script)\n",
    "# print(\"Local script 'setup_trino.sh' created.\")\n",
    "\n",
    "# # Step 3: Copy the script to your GCE instance.\n",
    "# !gcloud compute scp setup_trino.sh {GCE_INSTANCE_TRINO}:~/ --project={PROJECT_ID} --zone={GCE_ZONE_TRINO}\n",
    "# print(f\"Script copied to '{GCE_INSTANCE_TRINO}'.\")\n",
    "\n",
    "# # Step 4: Execute the script on the GCE instance.\n",
    "# !gcloud compute ssh {GCE_INSTANCE_TRINO} --project={PROJECT_ID} --zone={GCE_ZONE_TRINO} --command=\"chmod +x ~/setup_trino.sh && bash ~/setup_trino.sh\"\n",
    "# print(f\"Trino installation command executed on '{GCE_INSTANCE_TRINO}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query Data from Trino\n",
    "\n",
    "Now, you can connect to the Trino server and query the Iceberg table managed by BigLake Metastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Query from Trino\n",
    "# !gcloud compute ssh {GCE_INSTANCE_TRINO} --project={PROJECT_ID} --zone={GCE_ZONE_TRINO} --command=\"\"\"\n",
    "#     docker exec trino trino --catalog {ICEBERG_BIGLAKE_CATALOG} --schema {BIGLAKE_DATASET} --execute \"SHOW SCHEMAS;\"\n",
    "#     docker exec trino trino --catalog {ICEBERG_BIGLAKE_CATALOG} --schema {BIGLAKE_DATASET} --execute \"SHOW TABLES;\"\n",
    "#     docker exec trino trino --catalog {ICEBERG_BIGLAKE_CATALOG} --schema {BIGLAKE_DATASET} --execute \"SELECT * FROM {BIGLAKE_TABLE};\"\n",
    "# \"\"\"\n",
    "# print(f\"Queries executed on Trino instance '{GCE_INSTANCE_TRINO}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 8: Spark on GCE with BigLake Metastore\n",
    "\n",
    "This scenario demonstrates how to set up a Spark on a Google Compute Engine (GCE) virtual machine, install Spark using Docker, and configure it to access the BigLake Metastore via the Iceberg REST API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a GCE instance\n",
    "startup_script = \"\"\"#!/bin/bash\n",
    "sudo apt-get update\n",
    "sudo apt-get install -y apt-transport-https ca-certificates curl gnupg lsb-release\n",
    "curl -fsSL https://download.docker.com/linux/debian/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n",
    "echo \\\n",
    "  \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian \\\n",
    "  $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\n",
    "sudo apt-get update\n",
    "sudo apt-get install -y docker-ce docker-ce-cli containerd.io\n",
    "sudo systemctl start docker\n",
    "sudo systemctl enable docker\n",
    "sudo usermod -aG docker $USER\n",
    "\"\"\"\n",
    "\n",
    "# Write the script to a local file in your notebook environment\n",
    "with open(\"startup.sh\", \"w\") as f:\n",
    "    f.write(startup_script)\n",
    "\n",
    "print(\"Startup script saved to startup.sh\")\n",
    "\n",
    "# Create the GCE instance using the script file\n",
    "# This is the corrected command\n",
    "!gcloud compute instances create {GCE_INSTANCE_SPARK} \\\n",
    "    --project={PROJECT_ID} \\\n",
    "    --zone={GCE_ZONE_SPARK} \\\n",
    "    --machine-type=e2-medium \\\n",
    "    --image-family=debian-11 \\\n",
    "    --image-project=debian-cloud \\\n",
    "    --scopes=https://www.googleapis.com/auth/cloud-platform \\\n",
    "    --tags=http-server,https-server \\\n",
    "    --metadata-from-file=startup-script=startup.sh\n",
    "\n",
    "print(f\"GCE instance '{GCE_INSTANCE_SPARK}' creation initiated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install and Configure Spark with Docker\n",
    "\n",
    "Once the VM is running, SSH into it and install Docker. Then, configure Spark to use the BigLake Metastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the shell script for Spark setup.\n",
    "spark_setup_script = f\"\"\"#!/bin/bash\n",
    "set -e\n",
    "\n",
    "echo '--- SCRIPT START ---'\n",
    "\n",
    "# Pull the Spark Docker image\n",
    "sudo docker pull bitnami/spark:3.5\n",
    "\n",
    "# Stop and remove any existing Spark container\n",
    "sudo docker stop spark-biglake || true\n",
    "sudo docker rm spark-biglake || true\n",
    "\n",
    "# Run the Spark container in detached mode with proper GCP authentication\n",
    "sudo docker run -d --name spark-biglake \\\\\n",
    "    --user root \\\\\n",
    "    --network host \\\\\n",
    "    -e SPARK_HOME=/opt/bitnami/spark \\\\\n",
    "    -e JAVA_HOME=/opt/bitnami/java \\\\\n",
    "    -e HADOOP_USER_NAME=root \\\\\n",
    "    -e USER=root \\\\\n",
    "    -e HOME=/root \\\\\n",
    "    -e GOOGLE_CLOUD_PROJECT={PROJECT_ID} \\\\\n",
    "    -e HADOOP_CONF_DIR=/opt/bitnami/spark/conf \\\\\n",
    "    -e HADOOP_HOME=/opt/bitnami/spark \\\\\n",
    "    bitnami/spark:3.5\n",
    "\n",
    "# Setup directories, install packages, and test GCP authentication\n",
    "docker exec spark-biglake bash -c \"\n",
    "    # Create necessary directories\n",
    "    mkdir -p /tmp/.ivy2\n",
    "    chmod 777 /tmp/.ivy2\n",
    "\n",
    "    # Install required Python packages for OAuth2 authentication\n",
    "    echo 'Installing required Python packages...'\n",
    "    pip install google-auth google-auth-oauthlib google-auth-httplib2 || echo 'Package installation failed, continuing...'\n",
    "\"\n",
    "\n",
    "# Create a Python launcher script for interactive Spark session with OAuth2 authentication\n",
    "cat <<EOF > ~/spark_launcher.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "import code\n",
    "import google.auth\n",
    "from google.auth.transport.requests import Request\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def main():\n",
    "    print(\"=== Getting Google Cloud credentials ===\")\n",
    "\n",
    "    # Get default credentials and project\n",
    "    try:\n",
    "        credentials, project_id = google.auth.default()\n",
    "        credentials.refresh(Request())\n",
    "        access_token = credentials.token\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to get credentials: {{e}}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(\"=== Initializing Spark with BigLake/Iceberg Configuration ===\")\n",
    "\n",
    "    # Configuration\n",
    "    catalog_name = \"{ICEBERG_BIGLAKE_CATALOG}\"\n",
    "    warehouse_bucket = \"gs://{BUCKET_NAME}\"\n",
    "\n",
    "    # Create Spark session with OAuth2 token authentication\n",
    "    spark = SparkSession.builder \\\\\n",
    "        .appName(\"BigLake-Iceberg-Interactive\") \\\\\n",
    "        .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\\\n",
    "        .config(\"spark.sql.defaultCatalog\", catalog_name) \\\\\n",
    "        .config(f\"spark.sql.catalog.{{catalog_name}}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\\\n",
    "        .config(f\"spark.sql.catalog.{{catalog_name}}.type\", \"rest\") \\\\\n",
    "        .config(f\"spark.sql.catalog.{{catalog_name}}.uri\", \"https://biglake.googleapis.com/iceberg/v1beta/restcatalog\") \\\\\n",
    "        .config(f\"spark.sql.catalog.{{catalog_name}}.warehouse\", f\"{{warehouse_bucket}}/{{catalog_name}}\") \\\\\n",
    "        .config(f\"spark.sql.catalog.{{catalog_name}}.token\", access_token) \\\\\n",
    "        .config(f\"spark.sql.catalog.{{catalog_name}}.oauth2-server-uri\", \"https://oauth2.googleapis.com/token\") \\\\\n",
    "        .config(f\"spark.sql.catalog.{{catalog_name}}.header.x-goog-user-project\", project_id) \\\\\n",
    "        .config(f\"spark.sql.catalog.{{catalog_name}}.io-impl\", \"org.apache.iceberg.hadoop.HadoopFileIO\") \\\\\n",
    "        .config(f\"spark.sql.catalog.{{catalog_name}}.rest-metrics-reporting-enabled\", \"false\") \\\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    print(\"Spark session created successfully!\")\n",
    "    print(f\"Spark version: {{spark.version}}\")\n",
    "    print(f\"Application ID: {{spark.sparkContext.applicationId}}\")\n",
    "    print(\"\\\\n=== Interactive Spark Session Ready ===\")\n",
    "\n",
    "    # Make spark and sc available in the interactive session\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    # Start interactive Python shell with spark session available\n",
    "    code.interact(local=locals())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "EOF\n",
    "\n",
    "# Create a helper script to run the interactive Spark session\n",
    "cat <<EOF > ~/run_pyspark.sh\n",
    "#!/bin/bash\n",
    "\n",
    "echo \"Starting interactive Spark session with BigLake/Iceberg configuration...\"\n",
    "\n",
    "# Copy the launcher script into the container\n",
    "docker cp ~/spark_launcher.py spark-biglake:/tmp/spark_launcher.py\n",
    "\n",
    "# Run the launcher script using spark-submit with comprehensive GCP authentication\n",
    "docker exec -it spark-biglake /opt/bitnami/spark/bin/spark-submit \\\\\n",
    "    --packages org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.1,org.apache.iceberg:iceberg-gcp:1.6.1 \\\\\n",
    "    --conf spark.sql.adaptive.enabled=false \\\\\n",
    "    --conf spark.jars.ivy=/tmp/.ivy2 \\\\\n",
    "    --conf spark.hadoop.fs.gs.auth.service.account.enable=true \\\\\n",
    "    --conf spark.hadoop.google.cloud.auth.service.account.enable=true \\\\\n",
    "    --conf spark.hadoop.fs.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem \\\\\n",
    "    --conf spark.hadoop.fs.AbstractFileSystem.gs.impl=com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS \\\\\n",
    "    --conf spark.hadoop.security.authentication=simple \\\\\n",
    "    --conf spark.hadoop.security.authorization=false \\\\\n",
    "    --conf spark.sql.execution.arrow.pyspark.enabled=false \\\\\n",
    "    --driver-java-options \"-Dhadoop.home.dir=/opt/bitnami/spark -Duser.name=root -DHADOOP_USER_NAME=root -Dcom.google.cloud.hadoop.util.HttpTransportFactory.type=java_net -Djava.security.auth.login.config=/dev/null -Djava.security.krb5.conf=/dev/null -Dhadoop.security.authentication=simple -Dhadoop.security.authorization=false\" \\\\\n",
    "    --repositories https://repo1.maven.org/maven2/ \\\\\n",
    "    /tmp/spark_launcher.py\n",
    "EOF\n",
    "\n",
    "chmod +x ~/run_pyspark.sh\n",
    "\n",
    "echo '--- SCRIPT FINISHED ---'\n",
    "echo 'Spark container is running.'\n",
    "echo 'The interactive session will have spark and sc objects available.'\n",
    "\"\"\"\n",
    "\n",
    "# Step 2: Write the script to a local file.\n",
    "with open(\"setup_spark.sh\", \"w\") as f:\n",
    "    f.write(spark_setup_script)\n",
    "print(\"Local script 'setup_spark.sh' created.\")\n",
    "\n",
    "# Step 3: Copy the script to your GCE instance.\n",
    "!gcloud compute scp setup_spark.sh {GCE_INSTANCE_SPARK}:~/ --project={PROJECT_ID} --zone={GCE_ZONE_SPARK}\n",
    "print(f\"Script copied to '{GCE_INSTANCE_SPARK}'.\")\n",
    "\n",
    "# Step 4: Execute the script on the GCE instance.\n",
    "!gcloud compute ssh {GCE_INSTANCE_SPARK} --project={PROJECT_ID} --zone={GCE_ZONE_SPARK} --command=\\\"chmod +x ~/setup_spark.sh && bash ~/setup_spark.sh\\\"\n",
    "print(f\"Spark installation command executed on '{GCE_INSTANCE_SPARK}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Clean Up Resources\n",
    "\n",
    "Delete the Dataproc cluster and GCE instance to avoid charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the Dataproc cluster\n",
    "!gcloud dataproc clusters delete {CLUSTER_NAME} --region {REGION}\n",
    "\n",
    "# # Delete the GCE instance\n",
    "# !gcloud compute instances delete {GCE_INSTANCE_TRINO} --project={PROJECT_ID} --zone={GCE_ZONE_TRINO} --quiet\n",
    "# print(f\"GCE instance '{GCE_INSTANCE_TRINO}' deletion initiated.\")\n",
    "!gcloud compute instances delete {GCE_INSTANCE_SPARK} --project={PROJECT_ID} --zone={GCE_ZONE_SPARK} --quiet\n",
    "print(f\"GCE instance '{GCE_INSTANCE_SPARK}' deletion initiated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
