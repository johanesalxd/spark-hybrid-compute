{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicating a Single-Node Cloudera Environment with Dataproc (Manual Configuration)\n",
    "This notebook provides a definitive, step-by-step guide to creating a single-node Hadoop environment on Google Cloud Dataproc. This version explicitly follows the manual configuration for the BigQuery Metastore, bypassing the Dataproc Iceberg component to demonstrate the underlying setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configuration\n",
    "First, we define the configuration for our Dataproc cluster. **Make sure to replace the placeholder values** with your specific Google Cloud project details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Fill in these values before running!\n",
    "PROJECT_ID = \"your-gcp-project-id\"  # e.g., my-gcp-project\n",
    "REGION = \"your-gcp-region\"      # e.g., us-central1\n",
    "CLUSTER_NAME = \"my-single-node-cluster\"\n",
    "# A unique GCS bucket name. Using the project ID as a prefix is a good practice.\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-dataproc-bucket\"\n",
    "# A BigQuery dataset to act as a persistent Iceberg metastore.\n",
    "BQ_DATASET = \"my_iceberg_metastore\"\n",
    "# The name for our manually-configured catalog.\n",
    "CATALOG_NAME = \"biglake_manual\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2a: Create a Fully and Manually Configured Dataproc Cluster\n",
    "This is the most important step. We add a `--properties` flag to the cluster creation command to manually set all the required Spark configurations. This is the standard and correct way to configure the default Spark session for the entire cluster, which avoids all errors in the interactive notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the bucket exists and create it if it does not\n",
    "!gcloud storage buckets describe gs://{BUCKET_NAME} || gcloud storage buckets create gs://{BUCKET_NAME} --location={REGION}\n",
    "\n",
    "# Define the JARs and packages needed for the manual configuration\n",
    "PACKAGES = \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.6.1\"\n",
    "JARS = \"https://storage-download.googleapis.com/maven-central/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.6.1/iceberg-spark-runtime-3.5_2.12-1.6.1.jar,gs://spark-lib/bigquery/iceberg-bigquery-catalog-1.6.1-1.0.1-beta.jar\"\n",
    "\n",
    "# Define the properties for the Spark session in a readable, multi-line format\n",
    "properties_list = [\n",
    "    \"spark:spark.sql.warehouse.dir=hdfs:///user/hive/warehouse\",\n",
    "    f\"spark:spark.sql.catalog.{CATALOG_NAME}=org.apache.iceberg.spark.SparkCatalog\",\n",
    "    f\"spark:spark.sql.catalog.{CATALOG_NAME}.catalog-impl=org.apache.iceberg.gcp.bigquery.BigQueryMetastoreCatalog\",\n",
    "    f\"spark:spark.sql.catalog.{CATALOG_NAME}.gcp_project={PROJECT_ID}\",\n",
    "    f\"spark:spark.sql.catalog.{CATALOG_NAME}.location={REGION}\",\n",
    "    f\"spark:spark.sql.catalog.{CATALOG_NAME}.warehouse=gs://{BUCKET_NAME}/iceberg_warehouse_manual\"\n",
    "]\n",
    "PROPERTIES = \",\".join(properties_list)\n",
    "\n",
    "# Create the Dataproc cluster with all configurations set at creation time\n",
    "!gcloud dataproc clusters create {CLUSTER_NAME} \\\n",
    "    --region {REGION} \\\n",
    "    --single-node \\\n",
    "    --image-version 2.2-debian12 \\\n",
    "    --optional-components=JUPYTER \\\n",
    "    --enable-component-gateway \\\n",
    "    --bucket {BUCKET_NAME} \\\n",
    "    --packages={PACKAGES} \\\n",
    "    --jars={JARS} \\\n",
    "    --properties=\"{PROPERTIES}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2b: Update an Existing Cluster (Optional)\n",
    "If you already have a cluster running and want to apply or change the configuration without recreating it, you can use the `gcloud dataproc clusters update` command. Note that some properties may require a cluster restart to take effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the properties for the Spark session in a readable, multi-line format\n",
    "properties_list = [\n",
    "    \"spark:spark.sql.warehouse.dir=hdfs:///user/hive/warehouse\",\n",
    "    f\"spark:spark.sql.catalog.{CATALOG_NAME}=org.apache.iceberg.spark.SparkCatalog\",\n",
    "    f\"spark:spark.sql.catalog.{CATALOG_NAME}.catalog-impl=org.apache.iceberg.gcp.bigquery.BigQueryMetastoreCatalog\",\n",
    "    f\"spark:spark.sql.catalog.{CATALOG_NAME}.gcp_project={PROJECT_ID}\",\n",
    "    f\"spark:spark.sql.catalog.{CATALOG_NAME}.location={REGION}\",\n",
    "    f\"spark:spark.sql.catalog.{CATALOG_NAME}.warehouse=gs://{BUCKET_NAME}/iceberg_warehouse_manual\"\n",
    "]\n",
    "PROPERTIES = \",\".join(properties_list)\n",
    "\n",
    "# Update the cluster with the new properties\n",
    "!gcloud dataproc clusters update {CLUSTER_NAME} \\\n",
    "    --region {REGION} \\\n",
    "    --update-properties=\"{PROPERTIES}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Accessing the Jupyter Notebook\n",
    "Follow these steps to access the interactive Jupyter environment on the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Get the Pre-Configured Spark Session\n",
    "Because all configurations were set at the cluster level, we do not need to stop or configure the session. We simply get the default session that Jupyter started, which now has all our settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Get the existing, pre-configured Spark session. DO NOT use spark.stop()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Create a sample DataFrame that we will reuse across multiple steps.\n",
    "df = spark.createDataFrame(\n",
    "    [('Alice', 25), ('Bob', 30), ('Charlie', 35)], ['name', 'age'])\n",
    "\n",
    "print(\"Spark session is ready and sample DataFrame 'df' has been created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Write to HDFS\n",
    "Using the DataFrame created in the previous step, we will now write it to a new directory in HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir -p /user/my_data\n",
    "df.write.mode('overwrite').parquet('/user/my_data/people')\n",
    "!hdfs dfs -ls /user/my_data/people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Create a Hive Table\n",
    "The Hive warehouse location was correctly configured at the cluster level, so this will succeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS my_db\")\n",
    "spark.sql(\"USE my_db\")\n",
    "spark.sql(\"CREATE EXTERNAL TABLE IF NOT EXISTS people (name STRING, age INT) STORED AS PARQUET LOCATION '/user/my_data/people'\")\n",
    "spark.sql(\"SELECT * FROM my_db.people\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Create an Iceberg Table (using Hive Metastore)\n",
    "We can still use the default `spark_catalog` for Hive-based Iceberg tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iceberg_hive_table = \"spark_catalog.my_db.people_iceberg\"\n",
    "df.write.mode(\"overwrite\").format(\"iceberg\").save(iceberg_hive_table)\n",
    "spark.sql(f\"SELECT * FROM {iceberg_hive_table}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Create an Iceberg Table (using Manual BigLake Metastore)\n",
    "Now we use our manually configured catalog, `biglake_manual`, which was set at the cluster level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the BigQuery dataset that will act as the metastore\n",
    "!bq --location={REGION} mk --dataset {PROJECT_ID}:{BQ_DATASET}\n",
    "\n",
    "# Define the BigLake table name using our manual catalog\n",
    "biglake_table = f\"{CATALOG_NAME}.{BQ_DATASET}.people_biglake_manual\"\n",
    "\n",
    "# Save the DataFrame as an Iceberg table\n",
    "df.write.mode(\"overwrite\").format(\"iceberg\").save(biglake_table)\n",
    "\n",
    "# Query the table from Spark\n",
    "spark.sql(f\"SELECT * FROM {biglake_table}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Clean Up\n",
    "Finally, delete the cluster to avoid incurring ongoing charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud dataproc clusters delete {CLUSTER_NAME} --region {REGION}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
