{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicating a Single-Node Cloudera Environment with Dataproc (Manual Configuration)\n",
    "This notebook provides a definitive, step-by-step guide to creating a single-node Hadoop environment on Google Cloud Dataproc. This version explicitly follows the manual configuration for the BigQuery Metastore, bypassing the Dataproc Iceberg component to demonstrate the underlying setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configuration\n",
    "First, we define the configuration for our Dataproc cluster. **Make sure to replace the placeholder values** with your specific Google Cloud project details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Fill in these values before running!\n",
    "PROJECT_ID = \"johanesa-playground-326616\"  # e.g., my-gcp-project\n",
    "REGION = \"us-central1\"      # e.g., us-central1\n",
    "CLUSTER_NAME = \"my-single-node-dataproc-cluster\"\n",
    "# A unique GCS bucket name. Using the project ID as a prefix is a good practice.\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-dataproc-bucket\"\n",
    "# A BigQuery dataset to act as a persistent Iceberg metastore.\n",
    "BQ_DATASET = \"my_iceberg_metastore\"\n",
    "# Catalog Names\n",
    "ICEBERG_HIVE_CATALOG = \"iceberg_on_hive\"\n",
    "ICEBERG_BQ_CATALOG = \"iceberg_on_bq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This multi-line string contains the exact shell script content.\n",
    "init_script_content = \"\"\"#!/bin/bash\n",
    "# install-jars.sh\n",
    "\n",
    "# This script downloads required JARs for Iceberg + BigQuery Catalog integration.\n",
    "# It places them directly in Spark's classpath.\n",
    "\n",
    "set -e -x\n",
    "\n",
    "# Define variables for JARs\n",
    "ICEBERG_RUNTIME_URL=\"https://storage-download.googleapis.com/maven-central/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.6.1/iceberg-spark-runtime-3.5_2.12-1.6.1.jar\"\n",
    "BQ_CATALOG_JAR_GCS=\"gs://spark-lib/bigquery/iceberg-bigquery-catalog-1.6.1-1.0.1-beta.jar\"\n",
    "\n",
    "# Download the JARs directly into Spark's main jars directory\n",
    "wget -P /usr/lib/spark/jars/ \"$ICEBERG_RUNTIME_URL\"\n",
    "gsutil cp \"$BQ_CATALOG_JAR_GCS\" /usr/lib/spark/jars/\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the local and GCS paths for the script\n",
    "local_script_path = \"install-jars.sh\"\n",
    "gcs_script_path = f\"gs://{BUCKET_NAME}/scripts/{local_script_path}\"\n",
    "\n",
    "# Write the content to a local file\n",
    "with open(local_script_path, \"w\") as f:\n",
    "    f.write(init_script_content)\n",
    "\n",
    "print(f\"Initialization script created locally at: {local_script_path}\")\n",
    "\n",
    "# Upload the local script to GCS using a shell command\n",
    "!gsutil cp {local_script_path} {gcs_script_path}\n",
    "\n",
    "print(f\"Successfully uploaded script to: {gcs_script_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2a: Create a Fully and Manually Configured Dataproc Cluster\n",
    "This is the most important step. We add a `--properties` flag to the cluster creation command to manually set all the required Spark configurations. This is the standard and correct way to configure the default Spark session for the entire cluster, which avoids all errors in the interactive notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ONLY the catalog properties.\n",
    "properties_list = [\n",
    "    f\"spark:spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "    f\"spark:spark.sql.catalog.{ICEBERG_HIVE_CATALOG}=org.apache.iceberg.spark.SparkCatalog\",\n",
    "    f\"spark:spark.sql.catalog.{ICEBERG_HIVE_CATALOG}.type=hive\",\n",
    "    f\"spark:spark.sql.catalog.{ICEBERG_HIVE_CATALOG}.warehouse=gs://{BUCKET_NAME}/iceberg_on_hive\",\n",
    "    f\"spark:spark.sql.catalog.{ICEBERG_BQ_CATALOG}=org.apache.iceberg.spark.SparkCatalog\",\n",
    "    f\"spark:spark.sql.catalog.{ICEBERG_BQ_CATALOG}.catalog-impl=org.apache.iceberg.gcp.bigquery.BigQueryMetastoreCatalog\",\n",
    "    f\"spark:spark.sql.catalog.{ICEBERG_BQ_CATALOG}.gcp_project={PROJECT_ID}\",\n",
    "    f\"spark:spark.sql.catalog.{ICEBERG_BQ_CATALOG}.location={REGION}\",\n",
    "    f\"spark:spark.sql.catalog.{ICEBERG_BQ_CATALOG}.warehouse=gs://{BUCKET_NAME}/iceberg_on_bq\"\n",
    "]\n",
    "PROPERTIES = \",\".join(properties_list)\n",
    "\n",
    "# The final gcloud command to create the cluster\n",
    "# It references the GCS path of the script we just uploaded\n",
    "!gcloud dataproc clusters create {CLUSTER_NAME} \\\n",
    "    --project {PROJECT_ID} \\\n",
    "    --region {REGION} \\\n",
    "    --single-node \\\n",
    "    --image-version 2.2-debian12 \\\n",
    "    --optional-components=JUPYTER \\\n",
    "    --enable-component-gateway \\\n",
    "    --bucket {BUCKET_NAME} \\\n",
    "    --initialization-actions={gcs_script_path} \\\n",
    "    --properties=\"{PROPERTIES}\"\n",
    "\n",
    "print(f\"Cluster '{CLUSTER_NAME}' creation process initiated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Accessing the Jupyter Notebook\n",
    "Follow these steps to access the interactive Jupyter environment on the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Get the Pre-Configured Spark Session\n",
    "Because all configurations were set at the cluster level, we do not need to stop or configure the session. We simply get the default session that Jupyter started, which now has all our settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run a command to prove the catalog is working.\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Get the existing, pre-configured Spark session. DO NOT use spark.stop()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(\"Default Spark session is active and configured correctly. Ready to use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample DataFrame that we will reuse across multiple steps.\n",
    "df = spark.createDataFrame(\n",
    "    [('Alice', 25), ('Bob', 30), ('Charlie', 35)], ['name', 'age'])\n",
    "\n",
    "# Write data to HDFS as plain Parquet files\n",
    "df.write.mode('overwrite').parquet('/user/my_data/people')\n",
    "\n",
    "!hdfs dfs -ls /user/my_data/people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Create a Hive Table\n",
    "The Hive warehouse location was correctly configured at the cluster level, so this will succeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hive_db = \"my_hive_db\"\n",
    "hive_table = f\"{hive_db}.people_hive\"\n",
    "\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {hive_db} LOCATION 'hdfs:///user/hive_db'\")\n",
    "\n",
    "# First, drop the old table with the wrong schema\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {hive_table}\")\n",
    "\n",
    "# Re-create the table using BIGINT to match the data in the Parquet file\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE EXTERNAL TABLE IF NOT EXISTS {hive_table} (\n",
    "        name STRING,\n",
    "        age BIGINT\n",
    "    )\n",
    "    STORED AS PARQUET\n",
    "    LOCATION '/user/my_data/people'\n",
    "\"\"\")\n",
    "\n",
    "print(\"--- Standard Hive Table ---\")\n",
    "spark.sql(f\"SELECT * FROM {hive_table}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Create an Iceberg Table (using Hive Metastore)\n",
    "We can still use the default `spark_catalog` for Hive-based Iceberg tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verifying the pre-configured Iceberg catalog...\")\n",
    "spark.sql(f\"SHOW DATABASES IN {ICEBERG_HIVE_CATALOG}\").show()\n",
    "\n",
    "iceberg_hive_db = \"my_iceberg_db\"\n",
    "iceberg_hive_table = f\"{ICEBERG_HIVE_CATALOG}.{iceberg_hive_db}.people_iceberg\"\n",
    "\n",
    "spark.sql(\n",
    "    f\"CREATE DATABASE IF NOT EXISTS {ICEBERG_HIVE_CATALOG}.{iceberg_hive_db}\")\n",
    "df.write.format(\"iceberg\").mode(\"overwrite\").saveAsTable(iceberg_hive_table)\n",
    "\n",
    "print(\"\\n--- Iceberg Table on Internal Hive Metastore ---\")\n",
    "spark.sql(f\"SELECT * FROM {iceberg_hive_table}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Create an Iceberg Table (using Manual BigLake Metastore)\n",
    "Now we use our manually configured catalog, `biglake_manual`, which was set at the cluster level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verifying the pre-configured BQ catalog...\")\n",
    "spark.sql(f\"SHOW DATABASES IN {ICEBERG_BQ_CATALOG}\").show()\n",
    "\n",
    "# Create the BigQuery dataset that will act as the metastore\n",
    "!bq --location={REGION} mk --dataset {PROJECT_ID}:{BQ_DATASET}\n",
    "\n",
    "# Define the BigLake table name using our manual catalog\n",
    "biglake_table = f\"{ICEBERG_BQ_CATALOG}.{BQ_DATASET}.people_biglake_manual\"\n",
    "\n",
    "\n",
    "# Save the DataFrame as an Iceberg table\n",
    "df.write.mode(\"overwrite\").format(\"iceberg\").saveAsTable(biglake_table)\n",
    "\n",
    "# Query the table from Spark\n",
    "spark.sql(f\"SELECT * FROM {biglake_table}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Clean Up\n",
    "Finally, delete the cluster to avoid incurring ongoing charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud dataproc clusters delete {CLUSTER_NAME} --region {REGION}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
