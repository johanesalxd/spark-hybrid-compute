{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataproc with BigQuery Metastore Configuration\n",
    "A step-by-step guide to setting up a single-node Hadoop environment on Dataproc with a BigLake Metastore.\n",
    "\n",
    "**Note:** Steps 1 to 3 are executed in your local environment to provision the cluster. The subsequent steps are intended to be run within the Dataproc Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define Cluster Configuration\n",
    "First, set the configuration for your Dataproc cluster. **Replace the placeholder values** with your Google Cloud project details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Fill in these values before running!\n",
    "PROJECT_ID = \"johanesa-playground-326616\"  # Your Google Cloud project ID\n",
    "REGION = \"us-central1\"      # The region to create the cluster in\n",
    "CLUSTER_NAME = \"my-single-node-cluster\"  # A name for your Dataproc cluster\n",
    "# A unique GCS bucket name. Using the project ID as a prefix is a good practice.\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-dataproc-bucket\"\n",
    "\n",
    "# Local Hive\n",
    "HIVE_DB = \"my_hive_db\"\n",
    "HIVE_TABLE = f\"{HIVE_DB}.people_hive\"\n",
    "\n",
    "# Catalog for Iceberg on Hive\n",
    "ICEBERG_HIVE_CATALOG = \"iceberg_on_hive\"\n",
    "ICEBERG_HIVE_DB = \"my_iceberg_db\"\n",
    "ICEBERG_HIVE_TABLE = f\"{ICEBERG_HIVE_CATALOG}.{ICEBERG_HIVE_DB}.people_iceberg\"\n",
    "ICEBERG_HIVE_FROM_BQ = f\"{ICEBERG_HIVE_CATALOG}.{ICEBERG_HIVE_DB}.people_filtered_bq\"\n",
    "ICEBERG_HIVE_FROM_SPARK = f\"{ICEBERG_HIVE_CATALOG}.{ICEBERG_HIVE_DB}.people_filtered_spark\"\n",
    "\n",
    "# Catalog for Iceberg on BigLake Metastore\n",
    "BQ_DATASET = \"my_iceberg_metastore\"\n",
    "BQ_TABLE = f\"{BQ_DATASET}.people_biglake\"\n",
    "ICEBERG_BQ_CATALOG = \"iceberg_on_bq\"\n",
    "ICEBERG_BIGLAKE_TABLE = f\"{ICEBERG_BQ_CATALOG}.{BQ_TABLE}\"\n",
    "ICEBERG_BIGLAKE_FROM_SPARK = f\"{ICEBERG_BQ_CATALOG}.{BQ_DATASET}.people_filtered_spark\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create and Upload Initialization Script\n",
    "This script downloads the necessary JARs for Iceberg and BigQuery integration and places them in Spark's classpath. The script is then uploaded to a GCS bucket to be used during cluster creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This multi-line string contains the exact shell script content.\n",
    "init_script_content = \"\"\"#!/bin/bash\n",
    "# install-jars.sh\n",
    "\n",
    "# This script downloads required JARs for Iceberg + BigQuery Catalog integration.\n",
    "# It places them directly in Spark's classpath.\n",
    "\n",
    "set -e -x\n",
    "\n",
    "# Define variables for JARs\n",
    "ICEBERG_RUNTIME_URL=\"https://storage-download.googleapis.com/maven-central/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.6.1/iceberg-spark-runtime-3.5_2.12-1.6.1.jar\"\n",
    "BQ_CATALOG_JAR_GCS=\"gs://spark-lib/bigquery/iceberg-bigquery-catalog-1.6.1-1.0.1-beta.jar\"\n",
    "\n",
    "# Download the JARs directly into Spark's main jars directory\n",
    "wget -P /usr/lib/spark/jars/ \"$ICEBERG_RUNTIME_URL\"\n",
    "gsutil cp \"$BQ_CATALOG_JAR_GCS\" /usr/lib/spark/jars/\n",
    "\"\"\"\n",
    "# Define the local and GCS paths for the script\n",
    "local_script_path = \"install-jars.sh\"\n",
    "gcs_script_path = f\"gs://{BUCKET_NAME}/scripts/{local_script_path}\"\n",
    "\n",
    "# Write the content to a local file\n",
    "with open(local_script_path, \"w\") as f:\n",
    "    f.write(init_script_content)\n",
    "\n",
    "print(f\"Initialization script created locally at: {local_script_path}\")\n",
    "\n",
    "# Upload the local script to GCS using a shell command\n",
    "!gsutil cp {local_script_path} {gcs_script_path}\n",
    "\n",
    "print(f\"Successfully uploaded script to: {gcs_script_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create a custom Dataproc Cluster with Iceberg Catalog\n",
    "This step creates the Dataproc cluster with the `--properties` flag to manually set all required Spark configurations. This ensures the default Spark session for the entire cluster is correctly configured, preventing errors in the interactive notebook environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the catalog properties for the Dataproc cluster.\n",
    "properties_list = [\n",
    "    f\"spark:spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "    f\"spark:spark.sql.catalog.{ICEBERG_HIVE_CATALOG}=org.apache.iceberg.spark.SparkCatalog\",\n",
    "    f\"spark:spark.sql.catalog.{ICEBERG_HIVE_CATALOG}.type=hive\",\n",
    "    f\"spark:spark.sql.catalog.{ICEBERG_HIVE_CATALOG}.warehouse=gs://{BUCKET_NAME}/iceberg_on_hive\",\n",
    "    f\"spark:spark.sql.catalog.{ICEBERG_BQ_CATALOG}=org.apache.iceberg.spark.SparkCatalog\",\n",
    "    f\"spark:spark.sql.catalog.{ICEBERG_BQ_CATALOG}.catalog-impl=org.apache.iceberg.gcp.bigquery.BigQueryMetastoreCatalog\",\n",
    "    f\"spark:spark.sql.catalog.{ICEBERG_BQ_CATALOG}.gcp_project={PROJECT_ID}\",\n",
    "    f\"spark:spark.sql.catalog.{ICEBERG_BQ_CATALOG}.location={REGION}\",\n",
    "    f\"spark:spark.sql.catalog.{ICEBERG_BQ_CATALOG}.warehouse=gs://{BUCKET_NAME}/iceberg_on_bq\"\n",
    "]\n",
    "PROPERTIES = \",\".join(properties_list)\n",
    "\n",
    "# The final gcloud command to create the cluster\n",
    "# It references the GCS path of the script we just uploaded\n",
    "!gcloud dataproc clusters create {CLUSTER_NAME} \\\n",
    "    --project {PROJECT_ID} \\\n",
    "    --region {REGION} \\\n",
    "    --single-node \\\n",
    "    --image-version 2.2-debian12 \\\n",
    "    --optional-components=JUPYTER \\\n",
    "    --enable-component-gateway \\\n",
    "    --bucket {BUCKET_NAME} \\\n",
    "    --initialization-actions={gcs_script_path} \\\n",
    "    --properties=\"{PROPERTIES}\"\n",
    "\n",
    "print(f\"Cluster '{CLUSTER_NAME}' creation process initiated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Access Jupyter and Get Spark Session\n",
    "Once the cluster is running, access the Jupyter environment. Since all configurations were set at the cluster level, you can get the pre-configured Spark session without any additional setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the Spark session is active and configured correctly.\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Get the existing, pre-configured Spark session. DO NOT use spark.stop()\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(\"Default Spark session is active and configured correctly. Ready to use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create a Sample DataFrame and Write to HDFS\n",
    "Create a sample DataFrame and write it to HDFS as a Parquet file. This data will be used in subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample DataFrame that will be used throughout the notebook.\n",
    "df = spark.createDataFrame(\n",
    "    [('Alice', 25), ('Bob', 30), ('Charlie', 35)], ['name', 'age'])\n",
    "\n",
    "# Write the DataFrame to HDFS as a Parquet file.\n",
    "df.write.mode('overwrite').parquet('/user/my_data/people')\n",
    "\n",
    "# Verify that the Parquet file was created in HDFS.\n",
    "!hdfs dfs -ls /user/my_data/people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create a Hive Table\n",
    "Create a Hive table using the data stored in HDFS. The Hive warehouse location was configured at the cluster level, so this operation will succeed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new Hive database if it doesn't already exist.\n",
    "spark.sql(\n",
    "    f\"CREATE DATABASE IF NOT EXISTS {HIVE_DB} LOCATION 'hdfs:///user/hive_db'\")\n",
    "\n",
    "# Drop the table if it exists to ensure a clean slate.\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {HIVE_TABLE}\")\n",
    "\n",
    "# Create an external Hive table pointing to the Parquet file in HDFS.\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE EXTERNAL TABLE IF NOT EXISTS {HIVE_TABLE} (\n",
    "        name STRING,\n",
    "        age BIGINT\n",
    "    )\n",
    "    STORED AS PARQUET\n",
    "    LOCATION '/user/my_data/people'\n",
    "\"\"\")\n",
    "\n",
    "print(\"--- Standard Hive Table ---\")\n",
    "# Query the Hive table to verify its contents.\n",
    "spark.sql(f\"SELECT * FROM {HIVE_TABLE}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create an Iceberg Table with Hive Metastore\n",
    "Create an Iceberg table using the Hive metastore. The `iceberg_on_hive` is used for the Iceberg tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verifying the pre-configured Iceberg catalog...\")\n",
    "# Show the databases in the Hive Iceberg catalog.\n",
    "spark.sql(f\"SHOW DATABASES IN {ICEBERG_HIVE_CATALOG}\").show()\n",
    "\n",
    "# Create a new database in the Hive Iceberg catalog if it doesn't already exist.\n",
    "spark.sql(\n",
    "    f\"CREATE DATABASE IF NOT EXISTS {ICEBERG_HIVE_CATALOG}.{ICEBERG_HIVE_DB}\")\n",
    "# Write the DataFrame to a new Iceberg table in the Hive metastore.\n",
    "spark.sql(f\"SELECT * FROM {HIVE_TABLE}\") \\\n",
    "    .write \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(ICEBERG_HIVE_TABLE)\n",
    "\n",
    "print(\"\\n--- Iceberg Table on Internal Hive Metastore ---\")\n",
    "# Query the Iceberg table to verify its contents.\n",
    "spark.sql(f\"SELECT * FROM {ICEBERG_HIVE_TABLE}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create an Iceberg Table with BigLake Metastore\n",
    "Create an Iceberg table using the BigLake metastore. The `iceberg_on_bq` is used for the Iceberg tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verifying the pre-configured BQ catalog...\")\n",
    "# Show the databases in the BigQuery Iceberg catalog.\n",
    "spark.sql(f\"SHOW DATABASES IN {ICEBERG_BQ_CATALOG}\").show()\n",
    "\n",
    "# Create the BigQuery dataset that will act as the metastore\n",
    "!bq mk --connection --location={REGION} --project_id={PROJECT_ID} --connection_type=CLOUD_RESOURCE default-{REGION}\n",
    "!bq --location={REGION} mk --dataset {PROJECT_ID}:{BQ_DATASET}\n",
    "\n",
    "# Save the DataFrame to Iceberg table in the BigLake metastore.\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {ICEBERG_BIGLAKE_TABLE}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS\n",
    "  {ICEBERG_BIGLAKE_TABLE} ( name string,\n",
    "    age int )\n",
    "USING\n",
    "  ICEBERG TBLPROPERTIES ('bq_connection'='projects/{PROJECT_ID}/locations/{REGION}/connections/default-{REGION}');\n",
    "\"\"\")\n",
    "\n",
    "# Save the DataFrame as an Iceberg table in the BigLake metastore.\n",
    "spark.sql(f\"SELECT * FROM {ICEBERG_HIVE_TABLE}\") \\\n",
    "    .write \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(ICEBERG_BIGLAKE_TABLE)\n",
    "\n",
    "# Query the Iceberg table from Spark to verify its contents.\n",
    "# spark.sql(f\"SELECT * FROM {ICEBERG_BIGLAKE_TABLE}\").show()\n",
    "\n",
    "sql_query = f\"SELECT * FROM {BQ_TABLE}\"\n",
    "df = spark.read \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"viewsEnabled\", \"true\") \\\n",
    "    .option(\"query\", sql_query) \\\n",
    "    .option(\"materializationDataset\", BQ_DATASET) \\\n",
    "    .load()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Push Down Computation with BigQuery Connector using SQL\n",
    "Now, we'll use the BigQuery Connector to execute a raw SQL query directly within the BigQuery engine. The connector will send the query to BigQuery, which will process it, and only the final results will be written to Internal Hive Metastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the BigQuery SQL query as a string.\n",
    "# The entire query will be executed inside the BigQuery engine.\n",
    "bq_sql_query = f\"\"\"\n",
    "SELECT\n",
    "    name,\n",
    "    age\n",
    "FROM\n",
    "    {BQ_TABLE}\n",
    "WHERE\n",
    "    age > 28\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Sending this SQL query to BigQuery for execution ---\")\n",
    "print(bq_sql_query)\n",
    "\n",
    "# Use the 'bigquery' format with the 'query' option.\n",
    "# Spark sends the query to BigQuery and loads the result set.\n",
    "filtered_df = spark.read \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"viewsEnabled\", \"true\") \\\n",
    "    .option(\"query\", bq_sql_query) \\\n",
    "    .option(\"materializationDataset\", BQ_DATASET) \\\n",
    "    .load()\n",
    "\n",
    "print(f\"\\n--- Data returned from BigQuery query ---\")\n",
    "filtered_df.show()\n",
    "\n",
    "# Finally, save the small, filtered result set back to Internal Hive Metastore.\n",
    "filtered_df.write.format(\"iceberg\").mode(\n",
    "    \"overwrite\").saveAsTable(ICEBERG_HIVE_FROM_BQ)\n",
    "\n",
    "print(\"\\n--- Iceberg Table on Internal Hive Metastore ---\")\n",
    "spark.sql(f\"SELECT * FROM {ICEBERG_HIVE_FROM_BQ}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Push Down Computation with Serverless Spark\n",
    "Now, we'll use the Serverless Spark to execute a raw SQL query. The Serverless Spark which will process it, and only the final results will be written to Internal Hive Metastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Python script content for the Spark-native job\n",
    "pyspark_job_content = f\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# These values are configured via --properties in the gcloud command\n",
    "ICEBERG_BIGLAKE_TABLE = \"{ICEBERG_BIGLAKE_TABLE}\"\n",
    "\n",
    "# Define a new table name for the serverless job's output\n",
    "OUTPUT_TABLE = f\"{ICEBERG_BIGLAKE_FROM_SPARK}\"\n",
    "\n",
    "def main():\n",
    "    # A standard Spark session. Its catalog config will be injected by Dataproc.\n",
    "    spark = SparkSession.builder \\\\\n",
    "        .appName(\"Dataproc Serverless Spark Engine Filter\") \\\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # This is a Spark SQL query, not a BigQuery query.\n",
    "    # Spark reads the data from GCS and filters it in its own engine.\n",
    "    filter_query = f'''\n",
    "    SELECT\n",
    "        name,\n",
    "        age\n",
    "    FROM\n",
    "        {{ICEBERG_BIGLAKE_TABLE}}\n",
    "    WHERE\n",
    "        age > 28\n",
    "    '''\n",
    "\n",
    "    print(f\"--- Running this Spark SQL query: {{filter_query}} ---\")\n",
    "\n",
    "    filtered_df = spark.sql(filter_query)\n",
    "\n",
    "    print(\"--- Filtered data computed by Spark ---\")\n",
    "    filtered_df.show()\n",
    "\n",
    "    # Save the results to a different Iceberg table (one using the Hive catalog)\n",
    "    print(f\"--- Saving results to Iceberg table: {{OUTPUT_TABLE}} ---\")\n",
    "    filtered_df.write \\\\\n",
    "        .format(\"iceberg\") \\\\\n",
    "        .mode(\"overwrite\") \\\\\n",
    "        .saveAsTable(OUTPUT_TABLE)\n",
    "\n",
    "    print(\"Job completed successfully!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\"\n",
    "\n",
    "# Define local and GCS paths for the new job script\n",
    "local_job_path = \"filter_job_spark_sql.py\"\n",
    "gcs_job_path = f\"gs://{BUCKET_NAME}/scripts/{local_job_path}\"\n",
    "\n",
    "# Write and upload the script\n",
    "with open(local_job_path, \"w\") as f:\n",
    "    f.write(pyspark_job_content)\n",
    "\n",
    "!gsutil cp {local_job_path} {gcs_job_path}\n",
    "\n",
    "print(f\"Successfully uploaded Spark SQL job script to: {gcs_job_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the catalog properties for the serverless Dataproc cluster.\n",
    "properties_list = [\n",
    "    f\"spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "    f\"spark.sql.catalog.{ICEBERG_HIVE_CATALOG}=org.apache.iceberg.spark.SparkCatalog\",\n",
    "    f\"spark.sql.catalog.{ICEBERG_HIVE_CATALOG}.type=hive\",\n",
    "    f\"spark.sql.catalog.{ICEBERG_HIVE_CATALOG}.warehouse=gs://{BUCKET_NAME}/iceberg_on_hive\",\n",
    "    f\"spark.sql.catalog.{ICEBERG_BQ_CATALOG}=org.apache.iceberg.spark.SparkCatalog\",\n",
    "    f\"spark.sql.catalog.{ICEBERG_BQ_CATALOG}.catalog-impl=org.apache.iceberg.gcp.bigquery.BigQueryMetastoreCatalog\",\n",
    "    f\"spark.sql.catalog.{ICEBERG_BQ_CATALOG}.gcp_project={PROJECT_ID}\",\n",
    "    f\"spark.sql.catalog.{ICEBERG_BQ_CATALOG}.location={REGION}\",\n",
    "    f\"spark.sql.catalog.{ICEBERG_BQ_CATALOG}.warehouse=gs://{BUCKET_NAME}/iceberg_on_bq\"\n",
    "]\n",
    "PROPERTIES = \",\".join(properties_list)\n",
    "\n",
    "# Submit the PySpark job to a serverless Dataproc cluster.\n",
    "!gcloud dataproc batches submit pyspark {gcs_job_path} \\\n",
    "    --project={PROJECT_ID} \\\n",
    "    --region={REGION} \\\n",
    "    --batch=\"serverless-spark-engine-job\" \\\n",
    "    --version=\"2.2\" \\\n",
    "    --subnet=\"default\" \\\n",
    "    --jars=\"gs://spark-lib/bigquery/iceberg-bigquery-catalog-1.6.1-1.0.1-beta.jar,https://storage-download.googleapis.com/maven-central/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.6.1/iceberg-spark-runtime-3.5_2.12-1.6.1.jar\" \\\n",
    "    --properties=\"{PROPERTIES}\"\n",
    "\n",
    "print(\"\\nServerless batch job submitted. You can monitor its progress in the Google Cloud Console.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the table created by the serverless Spark job.\n",
    "print(f\"\\n--- Data returned from Spark query ---\")\n",
    "spark.sql(f\"SELECT * FROM {ICEBERG_BIGLAKE_FROM_SPARK}\").show()\n",
    "\n",
    "# Create a new Iceberg table from the results of the serverless Spark job.\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {ICEBERG_HIVE_FROM_SPARK}\n",
    "    USING iceberg\n",
    "    AS\n",
    "    SELECT * FROM {ICEBERG_BIGLAKE_FROM_SPARK}\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n--- Iceberg Table on Internal Hive Metastore ---\")\n",
    "spark.sql(f\"SELECT * FROM {ICEBERG_HIVE_FROM_SPARK}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Clean Up Resources\n",
    "Delete the Dataproc cluster to avoid incurring further charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the Dataproc cluster.\n",
    "!gcloud dataproc clusters delete {CLUSTER_NAME} --region {REGION}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
