{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataproc with BigLake Metastore\n",
    "A guide to setting up a single-node Dataproc cluster with a BigLake Metastore.\n",
    "\n",
    "**Note:** Steps 1-3 are for provisioning the cluster from your local environment. Subsequent steps are run in the Dataproc Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configure Cluster Settings\n",
    "Set the configuration for your Dataproc cluster. **Replace placeholders** with your Google Cloud project details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIG: Replace these values with your project details.\n",
    "PROJECT_ID = \"my-project-id\"  # Your Google Cloud project ID\n",
    "REGION = \"us-central1\"      # The region for the cluster\n",
    "CLUSTER_NAME = \"my-single-node-cluster\"  # A name for your Dataproc cluster\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-dataproc-bucket\"  # A unique GCS bucket name\n",
    "\n",
    "# Hive settings\n",
    "HIVE_DB = \"my_hive_db\"\n",
    "HIVE_TABLE = f\"{HIVE_DB}.people_hive\"\n",
    "\n",
    "# Iceberg on Hive settings\n",
    "ICEBERG_HIVE_CATALOG = \"iceberg_on_hive\"\n",
    "ICEBERG_HIVE_DB = \"my_iceberg_db\"\n",
    "ICEBERG_HIVE_TABLE = f\"{ICEBERG_HIVE_CATALOG}.{ICEBERG_HIVE_DB}.people_iceberg\"\n",
    "ICEBERG_HIVE_FROM_BQ = f\"{ICEBERG_HIVE_CATALOG}.{ICEBERG_HIVE_DB}.people_filtered_bq\"\n",
    "ICEBERG_HIVE_FROM_SPARK = f\"{ICEBERG_HIVE_CATALOG}.{ICEBERG_HIVE_DB}.people_filtered_spark\"\n",
    "\n",
    "# Iceberg on BigLake Metastore settings\n",
    "BQ_DATASET = \"my_iceberg_metastore\"\n",
    "BQ_TABLE = f\"{BQ_DATASET}.people_biglake\"\n",
    "ICEBERG_BQ_CATALOG = \"iceberg_on_bq\"\n",
    "ICEBERG_BIGLAKE_TABLE = f\"{ICEBERG_BQ_CATALOG}.{BQ_TABLE}\"\n",
    "ICEBERG_BIGLAKE_FROM_SPARK = f\"{ICEBERG_BQ_CATALOG}.{BQ_DATASET}.people_filtered_spark\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create and Upload Initialization Script\n",
    "This script downloads Iceberg and BigQuery JARs and places them in Spark's classpath. It's uploaded to a GCS bucket for use during cluster creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script downloads required JARs for Iceberg on BigLake Metastore integration.\n",
    "init_script_content = \"\"\"#!/bin/bash\n",
    "# install-jars.sh\n",
    "set -e -x\n",
    "\n",
    "# URLs for the JAR files\n",
    "ICEBERG_RUNTIME_URL=\"https://storage-download.googleapis.com/maven-central/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.6.1/iceberg-spark-runtime-3.5_2.12-1.6.1.jar\"\n",
    "BQ_CATALOG_JAR_GCS=\"gs://spark-lib/bigquery/iceberg-bigquery-catalog-1.6.1-1.0.1-beta.jar\"\n",
    "\n",
    "# Download JARs to Spark's classpath\n",
    "wget -P /usr/lib/spark/jars/ \"$ICEBERG_RUNTIME_URL\"\n",
    "gsutil cp \"$BQ_CATALOG_JAR_GCS\" /usr/lib/spark/jars/\n",
    "\"\"\"\n",
    "# Define local and GCS paths\n",
    "local_script_path = \"install-jars.sh\"\n",
    "gcs_script_path = f\"gs://{BUCKET_NAME}/scripts/{local_script_path}\"\n",
    "\n",
    "# Write the script to a local file\n",
    "with open(local_script_path, \"w\") as f:\n",
    "    f.write(init_script_content)\n",
    "\n",
    "print(f\"Initialization script created at: {local_script_path}\")\n",
    "\n",
    "# Upload the script to GCS\n",
    "!gsutil cp {local_script_path} {gcs_script_path}\n",
    "\n",
    "print(f\"Successfully uploaded script to: {gcs_script_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Dataproc Cluster with Iceberg Catalog\n",
    "This command creates a Dataproc cluster with the necessary Spark properties for Iceberg. Setting these at the cluster level ensures the Spark session is pre-configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define properties for the Dataproc cluster\n",
    "properties_list = [\n",
    "    \"spark:spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "    f\"spark:spark.sql.catalog.{ICEBERG_HIVE_CATALOG}=org.apache.iceberg.spark.SparkCatalog\",\n",
    "    f\"spark:spark.sql.catalog.{ICEBERG_HIVE_CATALOG}.type=hive\",\n",
    "    f\"spark:spark.sql.catalog.{ICEBERG_HIVE_CATALOG}.warehouse=gs://{BUCKET_NAME}/iceberg_on_hive\",\n",
    "    f\"spark:spark.sql.catalog.{ICEBERG_BQ_CATALOG}=org.apache.iceberg.spark.SparkCatalog\",\n",
    "    f\"spark:spark.sql.catalog.{ICEBERG_BQ_CATALOG}.catalog-impl=org.apache.iceberg.gcp.bigquery.BigQueryMetastoreCatalog\",\n",
    "    f\"spark:spark.sql.catalog.{ICEBERG_BQ_CATALOG}.gcp_project={PROJECT_ID}\",\n",
    "    f\"spark:spark.sql.catalog.{ICEBERG_BQ_CATALOG}.location={REGION}\",\n",
    "    f\"spark:spark.sql.catalog.{ICEBERG_BQ_CATALOG}.warehouse=gs://{BUCKET_NAME}/iceberg_on_bq\"\n",
    "]\n",
    "PROPERTIES = \",\".join(properties_list)\n",
    "\n",
    "# Create the Dataproc cluster\n",
    "!gcloud dataproc clusters create {CLUSTER_NAME} \\\n",
    "    --project {PROJECT_ID} \\\n",
    "    --region {REGION} \\\n",
    "    --single-node \\\n",
    "    --image-version 2.2-debian12 \\\n",
    "    --optional-components=JUPYTER \\\n",
    "    --enable-component-gateway \\\n",
    "    --bucket {BUCKET_NAME} \\\n",
    "    --initialization-actions={gcs_script_path} \\\n",
    "    --properties=\"{PROPERTIES}\"\n",
    "\n",
    "print(f\"Cluster '{CLUSTER_NAME}' creation initiated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Access Jupyter and Get Spark Session\n",
    "Once the cluster is running, access the Jupyter environment. The Spark session is pre-configured, so no extra setup is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the pre-configured Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "print(\"Spark session is active and ready to use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create a Sample DataFrame and Write to HDFS\n",
    "Create a sample DataFrame and write it to HDFS as a Parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample DataFrame\n",
    "df = spark.createDataFrame(\n",
    "    [('Alice', 25), ('Bob', 30), ('Charlie', 35)], ['name', 'age'])\n",
    "\n",
    "# Write the DataFrame to HDFS\n",
    "df.write.mode('overwrite').parquet('/user/my_data/people')\n",
    "\n",
    "# Verify the file was created in HDFS\n",
    "!hdfs dfs -ls /user/my_data/people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create a Hive Table\n",
    "Create a Hive table from the data in HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Hive database\n",
    "spark.sql(\n",
    "    f\"CREATE DATABASE IF NOT EXISTS {HIVE_DB} LOCATION 'hdfs:///user/hive_db'\")\n",
    "\n",
    "# Drop the table if it already exists\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {HIVE_TABLE}\")\n",
    "\n",
    "# Create an external Hive table pointing to the HDFS data\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE EXTERNAL TABLE IF NOT EXISTS {HIVE_TABLE} (\n",
    "        name STRING,\n",
    "        age BIGINT\n",
    "    )\n",
    "    STORED AS PARQUET\n",
    "    LOCATION '/user/my_data/people'\n",
    "\"\"\")\n",
    "\n",
    "print(\"--- Standard Hive Table ---\")\n",
    "# Query the Hive table\n",
    "spark.sql(f\"SELECT * FROM {HIVE_TABLE}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create an Iceberg Table with Hive Metastore\n",
    "Create an Iceberg table using the Hive metastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verifying the Iceberg catalog...\")\n",
    "# Show databases in the Hive Iceberg catalog\n",
    "spark.sql(f\"SHOW DATABASES IN {ICEBERG_HIVE_CATALOG}\").show()\n",
    "\n",
    "# Create a database in the Hive Iceberg catalog\n",
    "spark.sql(\n",
    "    f\"CREATE DATABASE IF NOT EXISTS {ICEBERG_HIVE_CATALOG}.{ICEBERG_HIVE_DB}\")\n",
    "\n",
    "# Write the DataFrame to an Iceberg table\n",
    "spark.sql(f\"SELECT * FROM {HIVE_TABLE}\") \\\n",
    "    .write \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(ICEBERG_HIVE_TABLE)\n",
    "\n",
    "print(\"--- Iceberg Table on Hive Metastore ---\")\n",
    "# Query the Iceberg table\n",
    "spark.sql(f\"SELECT * FROM {ICEBERG_HIVE_TABLE}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create an Iceberg Table with BigLake Metastore\n",
    "Create an Iceberg table using the BigLake metastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Verifying the BigQuery catalog...\")\n",
    "# Show databases in the BigQuery Iceberg catalog\n",
    "spark.sql(f\"SHOW DATABASES IN {ICEBERG_BQ_CATALOG}\").show()\n",
    "\n",
    "# Create the BigQuery dataset to act as the metastore\n",
    "!bq mk --connection --location={REGION} --project_id={PROJECT_ID} --connection_type=CLOUD_RESOURCE default-{REGION}\n",
    "!bq --location={REGION} mk --dataset {PROJECT_ID}:{BQ_DATASET}\n",
    "\n",
    "# Drop the table if it already exists\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {ICEBERG_BIGLAKE_TABLE}\")\n",
    "\n",
    "# Create the Iceberg table in the BigLake metastore\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS\n",
    "  {ICEBERG_BIGLAKE_TABLE} ( name string,\n",
    "    age int )\n",
    "USING\n",
    "  ICEBERG TBLPROPERTIES ('bq_connection'='projects/{PROJECT_ID}/locations/{REGION}/connections/default-{REGION}');\n",
    "\"\"\")\n",
    "\n",
    "# Save the DataFrame to the BigLake metastore\n",
    "spark.sql(f\"SELECT * FROM {ICEBERG_HIVE_TABLE}\") \\\n",
    "    .write \\\n",
    "    .format(\"iceberg\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(ICEBERG_BIGLAKE_TABLE)\n",
    "\n",
    "# Query the Iceberg table from Spark\n",
    "# spark.sql(f\"SELECT * FROM {ICEBERG_BIGLAKE_TABLE}\").show()\n",
    "\n",
    "sql_query = f\"SELECT * FROM {BQ_TABLE}\"\n",
    "df = spark.read \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"viewsEnabled\", \"true\") \\\n",
    "    .option(\"query\", sql_query) \\\n",
    "    .option(\"materializationDataset\", BQ_DATASET) \\\n",
    "    .load()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Push Down Computation to BigQuery\n",
    "Use the BigQuery Connector to execute a SQL query directly in BigQuery. Only the results are returned to Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This query is executed directly in BigQuery\n",
    "bq_sql_query = f\"\"\"\n",
    "SELECT\n",
    "    name,\n",
    "    age\n",
    "FROM\n",
    "    {BQ_TABLE}\n",
    "WHERE\n",
    "    age > 28\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Sending SQL query to BigQuery ---\")\n",
    "print(bq_sql_query)\n",
    "\n",
    "# Use the 'bigquery' format to send the query to BigQuery\n",
    "filtered_df = spark.read \\\n",
    "    .format(\"bigquery\") \\\n",
    "    .option(\"viewsEnabled\", \"true\") \\\n",
    "    .option(\"query\", bq_sql_query) \\\n",
    "    .option(\"materializationDataset\", BQ_DATASET) \\\n",
    "    .load()\n",
    "\n",
    "print(f\"--- Data returned from BigQuery ---\")\n",
    "filtered_df.show()\n",
    "\n",
    "# Save the filtered results to the Hive metastore\n",
    "filtered_df.write.format(\"iceberg\").mode(\n",
    "    \"overwrite\").saveAsTable(ICEBERG_HIVE_FROM_BQ)\n",
    "\n",
    "print(\"--- Iceberg Table on Hive Metastore ---\")\n",
    "spark.sql(f\"SELECT * FROM {ICEBERG_HIVE_FROM_BQ}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Push Down Computation with Serverless Spark\n",
    "Use Serverless Spark to execute a query. The results are written to the Hive metastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python script for the Spark job\n",
    "pyspark_job_content = f\"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# These values are injected by the gcloud command\n",
    "ICEBERG_BIGLAKE_TABLE = \"{ICEBERG_BIGLAKE_TABLE}\"\n",
    "OUTPUT_TABLE = f\"{ICEBERG_BIGLAKE_FROM_SPARK}\"\n",
    "\n",
    "def main():\n",
    "    spark = SparkSession.builder \\\\\n",
    "        .appName(\"Dataproc Serverless Spark Filter\") \\\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # This is a Spark SQL query, not a BigQuery query\n",
    "    filter_query = f'''\n",
    "    SELECT\n",
    "        name,\n",
    "        age\n",
    "    FROM\n",
    "        {{ICEBERG_BIGLAKE_TABLE}}\n",
    "    WHERE\n",
    "        age > 28\n",
    "    '''\n",
    "\n",
    "    print(f\"--- Running Spark SQL query: {{filter_query}} ---\")\n",
    "\n",
    "    filtered_df = spark.sql(filter_query)\n",
    "\n",
    "    print(\"--- Filtered data computed by Spark ---\")\n",
    "    filtered_df.show()\n",
    "\n",
    "    # Save results to a new Iceberg table\n",
    "    print(f\"--- Saving results to: {{OUTPUT_TABLE}} ---\")\n",
    "    filtered_df.write \\\\\n",
    "        .format(\"iceberg\") \\\\\n",
    "        .mode(\"overwrite\") \\\\\n",
    "        .saveAsTable(OUTPUT_TABLE)\n",
    "\n",
    "    print(\"Job completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\"\n",
    "\n",
    "# Define local and GCS paths for the job script\n",
    "local_job_path = \"filter_job_spark_sql.py\"\n",
    "gcs_job_path = f\"gs://{BUCKET_NAME}/scripts/{local_job_path}\"\n",
    "\n",
    "# Write and upload the script\n",
    "with open(local_job_path, \"w\") as f:\n",
    "    f.write(pyspark_job_content)\n",
    "\n",
    "!gsutil cp {local_job_path} {gcs_job_path}\n",
    "\n",
    "print(f\"Successfully uploaded Spark job script to: {gcs_job_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define properties for the serverless Dataproc job\n",
    "properties_list = [\n",
    "    \"spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\",\n",
    "    f\"spark.sql.catalog.{ICEBERG_HIVE_CATALOG}=org.apache.iceberg.spark.SparkCatalog\",\n",
    "    f\"spark.sql.catalog.{ICEBERG_HIVE_CATALOG}.type=hive\",\n",
    "    f\"spark.sql.catalog.{ICEBERG_HIVE_CATALOG}.warehouse=gs://{BUCKET_NAME}/iceberg_on_hive\",\n",
    "    f\"spark.sql.catalog.{ICEBERG_BQ_CATALOG}=org.apache.iceberg.spark.SparkCatalog\",\n",
    "    f\"spark.sql.catalog.{ICEBERG_BQ_CATALOG}.catalog-impl=org.apache.iceberg.gcp.bigquery.BigQueryMetastoreCatalog\",\n",
    "    f\"spark.sql.catalog.{ICEBERG_BQ_CATALOG}.gcp_project={PROJECT_ID}\",\n",
    "    f\"spark.sql.catalog.{ICEBERG_BQ_CATALOG}.location={REGION}\",\n",
    "    f\"spark.sql.catalog.{ICEBERG_BQ_CATALOG}.warehouse=gs://{BUCKET_NAME}/iceberg_on_bq\"\n",
    "]\n",
    "PROPERTIES = \",\".join(properties_list)\n",
    "\n",
    "# Submit the PySpark job to a serverless Dataproc cluster\n",
    "!gcloud dataproc batches submit pyspark {gcs_job_path} \\\n",
    "    --project={PROJECT_ID} \\\n",
    "    --region={REGION} \\\n",
    "    --batch=\"serverless-spark-engine-job\" \\\n",
    "    --version=\"2.2\" \\\n",
    "    --subnet=\"default\" \\\n",
    "    --jars=\"gs://spark-lib/bigquery/iceberg-bigquery-catalog-1.6.1-1.0.1-beta.jar,https://storage-download.googleapis.com/maven-central/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.6.1/iceberg-spark-runtime-3.5_2.12-1.6.1.jar\" \\\n",
    "    --properties=\"{PROPERTIES}\"\n",
    "\n",
    "print(\"Serverless batch job submitted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the table created by the serverless Spark job\n",
    "print(f\"\\n--- Data returned from Spark query ---\")\n",
    "spark.sql(f\"SELECT * FROM {ICEBERG_BIGLAKE_FROM_SPARK}\").show()\n",
    "\n",
    "# Create a new Iceberg table from the results\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {ICEBERG_HIVE_FROM_SPARK}\n",
    "    USING iceberg\n",
    "    AS\n",
    "    SELECT * FROM {ICEBERG_BIGLAKE_FROM_SPARK}\n",
    "\"\"\")\n",
    "\n",
    "print(\"--- Iceberg Table on Hive Metastore ---\")\n",
    "spark.sql(f\"SELECT * FROM {ICEBERG_HIVE_FROM_SPARK}\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Clean Up Resources\n",
    "Delete the Dataproc cluster to avoid charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the Dataproc cluster\n",
    "!gcloud dataproc clusters delete {CLUSTER_NAME} --region {REGION}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
