{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigLake Iceberg with Docker Setup\n",
    "\n",
    "This notebook demonstrates how to set up and use Apache Iceberg with Google Cloud BigLake metastore using Docker.\n",
    "\n",
    "## Overview\n",
    "- **BigLake**: Google Cloud's unified data lake solution\n",
    "- **Apache Iceberg**: Open table format for large analytic datasets\n",
    "- **Docker**: Containerized environment for consistent setup\n",
    "\n",
    "## Prerequisites\n",
    "- Docker and Docker Compose installed\n",
    "- Google Cloud Project with BigQuery enabled\n",
    "- Service account key file with appropriate permissions\n",
    "- GCS bucket for storing Iceberg data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration Setup\n",
    "\n",
    "First, let's configure all the necessary parameters for our BigLake Iceberg environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# =============================================================================\n",
    "# PROJECT CONFIGURATION\n",
    "# Replace these values with your actual project details\n",
    "# =============================================================================\n",
    "\n",
    "# Google Cloud Project Settings\n",
    "# TODO: Replace with your GCP project ID\n",
    "PROJECT_ID = \"my-project-id\"\n",
    "REGION = \"us-central1\"        # TODO: Replace with your preferred region\n",
    "\n",
    "# Google Cloud Storage bucket for Iceberg table data\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-docker-bucket\"  # Will be created if doesn't exist\n",
    "\n",
    "# =============================================================================\n",
    "# DOCKER CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "CONTAINER_NAME = \"biglake-iceberg-env\"  # Name for our Docker container\n",
    "JUPYTER_PORT = 8888                     # Port for Jupyter Lab access\n",
    "SPARK_UI_PORT = 4040                    # Port for Spark Web UI\n",
    "\n",
    "# =============================================================================\n",
    "# AUTHENTICATION CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Service account key file (must be in current directory)\n",
    "# TODO: Replace with your key file name\n",
    "GCP_KEY_FILE = \"my-project-id\"\n",
    "GCP_KEY_PATH = \"/home/jovyan/gcp-key.json\"  # Path inside Docker container\n",
    "\n",
    "# =============================================================================\n",
    "# BIGLAKE METASTORE CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "BIGLAKE_DATASET = \"my_iceberg_metastore\"  # BigQuery dataset for metastore\n",
    "BIGLAKE_CATALOG = \"iceberg_on_bq\"         # Iceberg catalog name\n",
    "BIGLAKE_CONNECTION = f\"projects/{PROJECT_ID}/locations/{REGION}/connections/default-{REGION}\"\n",
    "\n",
    "# Display configuration summary\n",
    "print(\"Configuration loaded successfully\")\n",
    "print(f\"Project: {PROJECT_ID}\")\n",
    "print(f\"Region: {REGION}\")\n",
    "print(f\"BigLake Dataset: {BIGLAKE_DATASET}\")\n",
    "print(f\"Iceberg Catalog: {BIGLAKE_CATALOG}\")\n",
    "print(f\"GCS Bucket: {BUCKET_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prerequisites Check\n",
    "\n",
    "Before setting up Docker, let's verify that all required components are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_prerequisites():\n",
    "    \"\"\"\n",
    "    Verify that Docker and GCP service account key are available.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If Docker is not installed or key file is missing\n",
    "    \"\"\"\n",
    "    print(\"Checking prerequisites...\")\n",
    "\n",
    "    # Check if Docker is installed and accessible\n",
    "    try:\n",
    "        result = subprocess.run([\"docker\", \"--version\"],\n",
    "                                check=True, capture_output=True, text=True)\n",
    "        print(f\"Docker is available: {result.stdout.strip()}\")\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        raise Exception(\"Docker is not installed or not in PATH\")\n",
    "\n",
    "    # Check if Docker Compose is available\n",
    "    try:\n",
    "        result = subprocess.run([\"docker-compose\", \"--version\"],\n",
    "                                check=True, capture_output=True, text=True)\n",
    "        print(f\"Docker Compose is available: {result.stdout.strip()}\")\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        raise Exception(\"Docker Compose is not installed or not in PATH\")\n",
    "\n",
    "    # Check if GCP service account key file exists\n",
    "    if not os.path.exists(GCP_KEY_FILE):\n",
    "        raise Exception(\n",
    "            f\"GCP key file '{GCP_KEY_FILE}' not found.\\n\"\n",
    "            f\"Please place your service account key file in the current directory.\"\n",
    "        )\n",
    "\n",
    "    print(f\"GCP service account key found: {GCP_KEY_FILE}\")\n",
    "    print(\"All prerequisites satisfied!\")\n",
    "\n",
    "\n",
    "# Run the prerequisites check\n",
    "check_prerequisites()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Docker Environment Setup\n",
    "\n",
    "Now we'll create the Docker configuration files and build our containerized environment.\n",
    "\n",
    "### What we're creating:\n",
    "- **Dockerfile**: Defines our container image with Spark, Iceberg, and BigLake dependencies\n",
    "- **docker-compose.yml**: Orchestrates the container with proper port mappings and volumes\n",
    "- **notebooks/**: Directory for Jupyter notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating Docker configuration files...\")\n",
    "\n",
    "# =============================================================================\n",
    "# CREATE DOCKERFILE\n",
    "# =============================================================================\n",
    "\n",
    "dockerfile_content = f\"\"\"\n",
    "# Base image with Jupyter and PySpark pre-installed\n",
    "FROM jupyter/pyspark-notebook:spark-3.5.0\n",
    "\n",
    "# Switch to root user to install system packages\n",
    "USER root\n",
    "\n",
    "# Install required system utilities\n",
    "RUN apt-get update && apt-get install -y wget curl && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Download required JAR files for BigLake + Iceberg integration\n",
    "# These JARs enable Spark to work with Iceberg tables and BigQuery metastore\n",
    "RUN wget -P /usr/local/spark/jars/ https://storage-download.googleapis.com/maven-central/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.6.1/iceberg-spark-runtime-3.5_2.12-1.6.1.jar && \\\\\n",
    "    wget -P /usr/local/spark/jars/ https://storage.googleapis.com/spark-lib/bigquery/iceberg-bigquery-catalog-1.6.1-1.0.1-beta.jar && \\\\\n",
    "    wget -P /usr/local/spark/jars/ https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-2.2.5.jar\n",
    "\n",
    "# Install Python packages for Google Cloud integration\n",
    "RUN pip install --no-cache-dir google-cloud-storage google-cloud-bigquery\n",
    "\n",
    "# Switch back to jovyan user (default Jupyter user)\n",
    "USER jovyan\n",
    "\n",
    "# Set Spark configuration for optimal performance\n",
    "ENV SPARK_OPTS=\"--driver-memory 4g --executor-memory 4g\"\n",
    "\"\"\"\n",
    "\n",
    "# Write Dockerfile to current directory\n",
    "with open(\"Dockerfile\", \"w\") as f:\n",
    "    f.write(dockerfile_content)\n",
    "\n",
    "print(\"Dockerfile created\")\n",
    "\n",
    "# =============================================================================\n",
    "# CREATE DOCKER-COMPOSE.YML\n",
    "# =============================================================================\n",
    "\n",
    "compose_content = f\"\"\"\n",
    "version: '3.8'\n",
    "services:\n",
    "  biglake-iceberg:\n",
    "    build: .                              # Build from local Dockerfile\n",
    "    container_name: {CONTAINER_NAME}      # Container name for easy reference\n",
    "    ports:\n",
    "      - \"{JUPYTER_PORT}:{JUPYTER_PORT}\"   # Jupyter Lab access\n",
    "      - \"{SPARK_UI_PORT}:{SPARK_UI_PORT}\" # Spark Web UI access\n",
    "    volumes:\n",
    "      - ./notebooks:/home/jovyan/work     # Mount notebooks directory\n",
    "      - ./{GCP_KEY_FILE}:/home/jovyan/gcp-key.json:ro  # Mount GCP key (read-only)\n",
    "    environment:\n",
    "      - JUPYTER_ENABLE_LAB=yes            # Enable Jupyter Lab interface\n",
    "      - GOOGLE_APPLICATION_CREDENTIALS=/home/jovyan/gcp-key.json  # GCP auth\n",
    "    working_dir: /home/jovyan/work        # Set working directory\n",
    "\"\"\"\n",
    "\n",
    "# Write docker-compose.yml to current directory\n",
    "with open(\"docker-compose.yml\", \"w\") as f:\n",
    "    f.write(compose_content)\n",
    "\n",
    "print(\"docker-compose.yml created\")\n",
    "\n",
    "# =============================================================================\n",
    "# CREATE NOTEBOOKS DIRECTORY\n",
    "# =============================================================================\n",
    "\n",
    "# Create notebooks directory if it doesn't exist\n",
    "os.makedirs(\"notebooks\", exist_ok=True)\n",
    "print(\"notebooks/ directory created\")\n",
    "\n",
    "print(\"Docker configuration completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Build and Start Docker Container\n",
    "\n",
    "Now we'll build the Docker image and start the container. This process will:\n",
    "1. Download the base Jupyter/PySpark image\n",
    "2. Install required JAR files for Iceberg and BigLake\n",
    "3. Start the container with Jupyter Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# BUILD DOCKER IMAGE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Building Docker image... (this may take a few minutes)\")\n",
    "print(\"Downloading base image and dependencies...\")\n",
    "\n",
    "# Build the Docker image using docker-compose\n",
    "result = subprocess.run([\"docker-compose\", \"build\"],\n",
    "                        capture_output=True, text=True)\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(f\"Build failed: {result.stderr}\")\n",
    "    raise Exception(\"Docker build failed\")\n",
    "\n",
    "print(\"Docker image built successfully\")\n",
    "\n",
    "# =============================================================================\n",
    "# START CONTAINER\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Starting container...\")\n",
    "\n",
    "# Start the container in detached mode\n",
    "result = subprocess.run([\"docker-compose\", \"up\", \"-d\"],\n",
    "                        capture_output=True, text=True)\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(f\"Container start failed: {result.stderr}\")\n",
    "    raise Exception(\"Container start failed\")\n",
    "\n",
    "print(\"Container started successfully\")\n",
    "\n",
    "# =============================================================================\n",
    "# WAIT FOR CONTAINER TO BE READY\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Waiting for Jupyter Lab to initialize...\")\n",
    "time.sleep(10)  # Give container time to start up\n",
    "\n",
    "# =============================================================================\n",
    "# GET JUPYTER ACCESS URL\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Retrieving Jupyter Lab access URL...\")\n",
    "\n",
    "# Get container logs to find Jupyter URL with token\n",
    "result = subprocess.run([\"docker\", \"logs\", CONTAINER_NAME],\n",
    "                        capture_output=True, text=True)\n",
    "logs = result.stdout\n",
    "\n",
    "# Extract Jupyter URL from logs\n",
    "jupyter_url = None\n",
    "for line in logs.split('\\n'):\n",
    "    if 'http://127.0.0.1:8888/lab?token=' in line:\n",
    "        jupyter_url = line.strip()\n",
    "        break\n",
    "\n",
    "# Display access information\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CONTAINER READY!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if jupyter_url:\n",
    "    print(f\"Jupyter Lab: {jupyter_url}\")\n",
    "else:\n",
    "    print(f\"Jupyter Lab: http://localhost:{JUPYTER_PORT}\")\n",
    "    print(\"If URL doesn't work, check container logs for the token\")\n",
    "\n",
    "print(f\"Container: {CONTAINER_NAME}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Open Jupyter Lab in your browser\")\n",
    "print(\"2. Create a new notebook in the 'work' directory\")\n",
    "print(\"3. Copy and run the Spark configuration code from the next cells\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Spark Session Configuration\n",
    "\n",
    "**⚠️ IMPORTANT: Run this code inside the Jupyter container**\n",
    "\n",
    "Copy the following code to a new notebook cell in Jupyter Lab (running inside the Docker container).\n",
    "\n",
    "This configures Spark to work with BigLake metastore and Iceberg tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COPY THIS CODE TO JUPYTER LAB (INSIDE DOCKER CONTAINER)\n",
    "# =============================================================================\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Configuration - Update these values to match your setup\n",
    "# TODO: Your Google Cloud project ID\n",
    "PROJECT_ID = \"my-project-id\"\n",
    "REGION = \"us-central1\"                    # TODO: Your region\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-docker-bucket\"  # GCS bucket for Iceberg data\n",
    "BIGLAKE_DATASET = \"my_iceberg_metastore\"  # BigQuery dataset for metastore\n",
    "BIGLAKE_CATALOG = \"iceberg_on_bq\"         # Iceberg catalog name\n",
    "GCP_KEY_PATH = \"/home/jovyan/gcp-key.json\"  # Service account key path\n",
    "\n",
    "print(\"Configuring Spark session for BigLake Iceberg...\")\n",
    "\n",
    "# Create Spark session with BigLake and Iceberg configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BigLake_Iceberg_Demo\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(f\"spark.sql.catalog.{BIGLAKE_CATALOG}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(f\"spark.sql.catalog.{BIGLAKE_CATALOG}.catalog-impl\", \"org.apache.iceberg.gcp.bigquery.BigQueryMetastoreCatalog\") \\\n",
    "    .config(f\"spark.sql.catalog.{BIGLAKE_CATALOG}.gcp_project\", PROJECT_ID) \\\n",
    "    .config(f\"spark.sql.catalog.{BIGLAKE_CATALOG}.location\", REGION) \\\n",
    "    .config(f\"spark.sql.catalog.{BIGLAKE_CATALOG}.warehouse\", f\"gs://{BUCKET_NAME}/{BIGLAKE_CATALOG}\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", GCP_KEY_PATH) \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session with BigLake catalog created successfully!\")\n",
    "print(f\"Spark UI: http://localhost:4040\")\n",
    "print(f\"Catalog: {BIGLAKE_CATALOG}\")\n",
    "print(f\"Dataset: {BIGLAKE_DATASET}\")\n",
    "\n",
    "# =============================================================================\n",
    "# CREATE SAMPLE DATA FOR TESTING\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nCreating sample dataset...\")\n",
    "\n",
    "# Sample employee data for demonstration\n",
    "sample_data = [\n",
    "    (\"Alice\", 25, \"Engineering\", 75000),\n",
    "    (\"Bob\", 30, \"Marketing\", 65000),\n",
    "    (\"Charlie\", 35, \"Engineering\", 85000),\n",
    "    (\"Diana\", 28, \"Sales\", 60000),\n",
    "    (\"Eve\", 32, \"Engineering\", 90000)\n",
    "]\n",
    "\n",
    "# Create DataFrame with proper schema\n",
    "df = spark.createDataFrame(\n",
    "    sample_data, [\"name\", \"age\", \"department\", \"salary\"]\n",
    ")\n",
    "\n",
    "print(\"Sample DataFrame created:\")\n",
    "df.show()\n",
    "\n",
    "print(\"Ready to work with BigLake Iceberg tables!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. BigLake Iceberg Table Operations\n",
    "\n",
    "**⚠️ IMPORTANT: Also run this code inside the Jupyter container**\n",
    "\n",
    "This section demonstrates how to:\n",
    "1. Create Iceberg tables in BigLake metastore\n",
    "2. Insert data into Iceberg tables\n",
    "3. Query Iceberg tables\n",
    "4. View table metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# COPY THIS CODE TO JUPYTER LAB (INSIDE DOCKER CONTAINER)\n",
    "# =============================================================================\n",
    "\n",
    "# Table configuration\n",
    "TABLE_NAME = f\"{BIGLAKE_CATALOG}.{BIGLAKE_DATASET}.employees\"\n",
    "\n",
    "print(\"Working with BigLake Iceberg tables...\")\n",
    "print(f\"Table: {TABLE_NAME}\")\n",
    "\n",
    "# =============================================================================\n",
    "# STEP 1: CREATE ICEBERG TABLE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nCreating Iceberg table in BigLake metastore...\")\n",
    "\n",
    "try:\n",
    "    # Drop table if it exists (for demo purposes)\n",
    "    print(\"Dropping existing table (if any)...\")\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {TABLE_NAME}\")\n",
    "\n",
    "    # Create new Iceberg table with schema\n",
    "    print(\"Creating new Iceberg table...\")\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE {TABLE_NAME} (\n",
    "        name STRING,\n",
    "        age INT,\n",
    "        department STRING,\n",
    "        salary BIGINT\n",
    "    )\n",
    "    USING ICEBERG\n",
    "    TBLPROPERTIES (\n",
    "        'bq_connection'='projects/{PROJECT_ID}/locations/{REGION}/connections/default-{REGION}'\n",
    "    )\n",
    "    \"\"\")\n",
    "\n",
    "    print(f\"Iceberg table created successfully: {TABLE_NAME}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Table creation failed: {e}\")\n",
    "    print(\"Make sure:\")\n",
    "    print(\"   - BigQuery dataset exists\")\n",
    "    print(\"   - BigQuery connection exists\")\n",
    "    print(\"   - Service account has proper permissions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 2: INSERT SAMPLE DATA\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nInserting sample data into Iceberg table...\")\n",
    "\n",
    "try:\n",
    "    # Insert DataFrame data into Iceberg table\n",
    "    df.write \\\n",
    "        .format(\"iceberg\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(TABLE_NAME)\n",
    "\n",
    "    print(\"Data inserted successfully\")\n",
    "    print(f\"Inserted {df.count()} records\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Data insertion failed: {e}\")\n",
    "    print(\"Check GCS bucket permissions and connectivity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 3: QUERY THE ICEBERG TABLE\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nQuerying the Iceberg table...\")\n",
    "\n",
    "try:\n",
    "    # Query 1: Show all records\n",
    "    print(\"\\n--- All Records ---\")\n",
    "    result = spark.sql(f\"SELECT * FROM {TABLE_NAME}\")\n",
    "    result.show()\n",
    "\n",
    "    # Query 2: Filter by department\n",
    "    print(\"\\n--- Engineering Department (Sorted by Salary) ---\")\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT name, age, salary\n",
    "        FROM {TABLE_NAME}\n",
    "        WHERE department = 'Engineering'\n",
    "        ORDER BY salary DESC\n",
    "    \"\"\").show()\n",
    "\n",
    "    # Query 3: Department summary with aggregations\n",
    "    print(\"\\n--- Department Summary ---\")\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT department,\n",
    "               COUNT(*) as employee_count,\n",
    "               AVG(salary) as avg_salary,\n",
    "               MAX(salary) as max_salary,\n",
    "               MIN(salary) as min_salary\n",
    "        FROM {TABLE_NAME}\n",
    "        GROUP BY department\n",
    "        ORDER BY avg_salary DESC\n",
    "    \"\"\").show()\n",
    "\n",
    "    print(\"All queries executed successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Query failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 4: SHOW TABLE METADATA\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nTable Metadata and Information:\")\n",
    "\n",
    "try:\n",
    "    # Show table schema\n",
    "    print(\"\\n--- Table Schema ---\")\n",
    "    spark.sql(f\"DESCRIBE {TABLE_NAME}\").show()\n",
    "\n",
    "    # Show table properties\n",
    "    print(\"\\n--- Table Properties ---\")\n",
    "    spark.sql(f\"SHOW TBLPROPERTIES {TABLE_NAME}\").show()\n",
    "\n",
    "    # Show table location and format\n",
    "    print(\"\\n--- Table Details ---\")\n",
    "    spark.sql(f\"DESCRIBE EXTENDED {TABLE_NAME}\").show(truncate=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Metadata query failed: {e}\")\n",
    "\n",
    "print(\"\\nBigLake Iceberg demo completed successfully!\")\n",
    "print(f\"Table: {TABLE_NAME}\")\n",
    "print(f\"You can now use this table for your data operations\")\n",
    "print(f\"The table is stored in GCS: gs://{BUCKET_NAME}/{BIGLAKE_CATALOG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CLEANUP RESOURCES\n",
    "# Uncomment the lines below to clean up when you're done\n",
    "# =============================================================================\n",
    "\n",
    "# Stop Spark session\n",
    "# spark.stop()\n",
    "# print(\"Spark session stopped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cleanup (Optional)\n",
    "\n",
    "When you're done with the demo, you can clean up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CLEANUP RESOURCES\n",
    "# Uncomment the lines below to clean up when you're done\n",
    "# =============================================================================\n",
    "\n",
    "# Stop and remove Docker containers\n",
    "# import subprocess\n",
    "# print(\"Stopping Docker containers...\")\n",
    "# subprocess.run([\"docker-compose\", \"down\"], capture_output=True)\n",
    "# print(\"Docker containers stopped\")\n",
    "\n",
    "# To remove all data and volumes:\n",
    "# subprocess.run([\"docker-compose\", \"down\", \"-v\"], capture_output=True)\n",
    "# print(\"All data and volumes removed\")\n",
    "\n",
    "print(\"Cleanup commands are commented out for safety\")\n",
    "print(\"Uncomment the lines above to clean up resources when done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
