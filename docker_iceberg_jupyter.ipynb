{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec553f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 1: Configuration\n",
    "# =============================================================================\n",
    "# Replace these values with your project details\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# GCP Project Configuration\n",
    "PROJECT_ID = \"my-project-id\"  # Your Google Cloud project ID\n",
    "REGION = \"us-central1\"        # The region for BigQuery/GCS\n",
    "# GCS bucket for Iceberg data\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-docker-bucket\"\n",
    "\n",
    "# Docker Settings\n",
    "CONTAINER_NAME = \"biglake-iceberg-env\"\n",
    "JUPYTER_PORT = 8888\n",
    "SPARK_UI_PORT = 4040\n",
    "# Place your service account key here\n",
    "GCP_KEY_FILE = \"my-project-id.json\"\n",
    "GCP_KEY_PATH = \"/home/jovyan/gcp-key.json\"  # Path inside the Docker container\n",
    "\n",
    "# BigLake Metastore Settings\n",
    "BIGLAKE_DATASET = \"my_iceberg_metastore\"  # BigQuery dataset for metastore\n",
    "BIGLAKE_CATALOG = \"iceberg_on_bq\"  # Iceberg catalog name\n",
    "BIGLAKE_CONNECTION = f\"projects/{PROJECT_ID}/locations/{REGION}/connections/default-{REGION}\"\n",
    "\n",
    "print(\"‚úÖ Configuration loaded\")\n",
    "print(f\"Project: {PROJECT_ID}\")\n",
    "print(f\"BigLake Dataset: {BIGLAKE_DATASET}\")\n",
    "print(f\"Iceberg Catalog: {BIGLAKE_CATALOG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9fe4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 2: Docker Environment Setup\n",
    "# =============================================================================\n",
    "# Creates minimal Docker environment for BigLake + Iceberg\n",
    "\n",
    "def check_prerequisites():\n",
    "    \"\"\"Check if Docker and GCP key file are available\"\"\"\n",
    "    try:\n",
    "        subprocess.run([\"docker\", \"--version\"],\n",
    "                       check=True, capture_output=True)\n",
    "        print(\"‚úÖ Docker is available\")\n",
    "    except (subprocess.CalledProcessError, FileNotFoundError):\n",
    "        raise Exception(\"‚ùå Docker is not installed or not in PATH\")\n",
    "\n",
    "    if not os.path.exists(GCP_KEY_FILE):\n",
    "        raise Exception(\n",
    "            f\"‚ùå GCP key file '{GCP_KEY_FILE}' not found. Please place your service account key file here.\")\n",
    "\n",
    "    print(f\"‚úÖ GCP key file found: {GCP_KEY_FILE}\")\n",
    "\n",
    "\n",
    "check_prerequisites()\n",
    "\n",
    "# Create minimal Dockerfile for BigLake + Iceberg\n",
    "dockerfile_content = f\"\"\"\n",
    "FROM jupyter/pyspark-notebook:spark-3.5.0\n",
    "\n",
    "# Switch to root to install packages\n",
    "USER root\n",
    "\n",
    "# Install required system packages\n",
    "RUN apt-get update && apt-get install -y wget curl && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Download only required JARs for BigLake + Iceberg\n",
    "RUN wget -P /usr/local/spark/jars/ https://storage-download.googleapis.com/maven-central/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.6.1/iceberg-spark-runtime-3.5_2.12-1.6.1.jar && \\\\\n",
    "    wget -P /usr/local/spark/jars/ https://storage.googleapis.com/spark-lib/bigquery/iceberg-bigquery-catalog-1.6.1-1.0.1-beta.jar && \\\\\n",
    "    wget -P /usr/local/spark/jars/ https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-2.2.5.jar\n",
    "\n",
    "# Install Python packages for GCP integration\n",
    "RUN pip install --no-cache-dir google-cloud-storage google-cloud-bigquery\n",
    "\n",
    "# Switch back to jovyan user\n",
    "USER jovyan\n",
    "\n",
    "# Set environment variables for BigLake\n",
    "ENV SPARK_OPTS=\"--driver-memory 4g --executor-memory 4g\"\n",
    "\"\"\"\n",
    "\n",
    "# Write Dockerfile\n",
    "with open(\"Dockerfile\", \"w\") as f:\n",
    "    f.write(dockerfile_content)\n",
    "\n",
    "print(\"‚úÖ Dockerfile created\")\n",
    "\n",
    "# Create docker-compose.yml\n",
    "compose_content = f\"\"\"\n",
    "version: '3.8'\n",
    "services:\n",
    "  biglake-iceberg:\n",
    "    build: .\n",
    "    container_name: {CONTAINER_NAME}\n",
    "    ports:\n",
    "      - \"{JUPYTER_PORT}:{JUPYTER_PORT}\"\n",
    "      - \"{SPARK_UI_PORT}:{SPARK_UI_PORT}\"\n",
    "    volumes:\n",
    "      - ./notebooks:/home/jovyan/work\n",
    "      - ./{GCP_KEY_FILE}:/home/jovyan/gcp-key.json:ro\n",
    "    environment:\n",
    "      - JUPYTER_ENABLE_LAB=yes\n",
    "      - GOOGLE_APPLICATION_CREDENTIALS=/home/jovyan/gcp-key.json\n",
    "    working_dir: /home/jovyan/work\n",
    "\"\"\"\n",
    "\n",
    "with open(\"docker-compose.yml\", \"w\") as f:\n",
    "    f.write(compose_content)\n",
    "\n",
    "print(\"‚úÖ docker-compose.yml created\")\n",
    "\n",
    "# Create notebooks directory\n",
    "os.makedirs(\"notebooks\", exist_ok=True)\n",
    "\n",
    "# Build and start the container\n",
    "print(\"üê≥ Building Docker image... (this may take a few minutes)\")\n",
    "result = subprocess.run([\"docker-compose\", \"build\"],\n",
    "                        capture_output=True, text=True)\n",
    "if result.returncode != 0:\n",
    "    print(f\"‚ùå Build failed: {result.stderr}\")\n",
    "    raise Exception(\"Docker build failed\")\n",
    "\n",
    "print(\"üöÄ Starting container...\")\n",
    "result = subprocess.run([\"docker-compose\", \"up\", \"-d\"],\n",
    "                        capture_output=True, text=True)\n",
    "if result.returncode != 0:\n",
    "    print(f\"‚ùå Container start failed: {result.stderr}\")\n",
    "    raise Exception(\"Container start failed\")\n",
    "\n",
    "# Wait for container to be ready\n",
    "print(\"‚è≥ Waiting for container to be ready...\")\n",
    "time.sleep(10)\n",
    "\n",
    "# Get Jupyter URL\n",
    "result = subprocess.run([\"docker\", \"logs\", CONTAINER_NAME],\n",
    "                        capture_output=True, text=True)\n",
    "logs = result.stdout\n",
    "\n",
    "# Extract Jupyter URL\n",
    "for line in logs.split('\\n'):\n",
    "    if 'http://127.0.0.1:8888/lab?token=' in line:\n",
    "        jupyter_url = line.strip()\n",
    "        print(f\"‚úÖ Jupyter Lab is ready!\")\n",
    "        print(f\"üîó Access at: {jupyter_url}\")\n",
    "        break\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Jupyter URL not found in logs. Container may still be starting.\")\n",
    "    print(f\"üîó Try accessing: http://localhost:{JUPYTER_PORT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99de8db3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6daaddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 3: Spark Session + Sample Data\n",
    "# =============================================================================\n",
    "# Initialize Spark with BigLake catalog and create sample data\n",
    "# Run this inside the Jupyter container (copy to notebook cell)\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Configuration (these should match your settings from Cell 1)\n",
    "PROJECT_ID = \"my-project-id\"  # Your Google Cloud project ID\n",
    "REGION = \"us-central1\"        # The region for BigQuery/GCS\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-docker-bucket\"\n",
    "BIGLAKE_DATASET = \"my_iceberg_metastore\"  # BigQuery dataset for metastore\n",
    "BIGLAKE_CATALOG = \"iceberg_on_bq\"  # Iceberg catalog name\n",
    "GCP_KEY_PATH = \"/home/jovyan/gcp-key.json\"  # Path inside the Docker container\n",
    "\n",
    "# Create Spark session configured for BigLake metastore\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BigLake_Iceberg_Demo\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(f\"spark.sql.catalog.{BIGLAKE_CATALOG}\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(f\"spark.sql.catalog.{BIGLAKE_CATALOG}.catalog-impl\", \"org.apache.iceberg.gcp.bigquery.BigQueryMetastoreCatalog\") \\\n",
    "    .config(f\"spark.sql.catalog.{BIGLAKE_CATALOG}.gcp_project\", PROJECT_ID) \\\n",
    "    .config(f\"spark.sql.catalog.{BIGLAKE_CATALOG}.location\", REGION) \\\n",
    "    .config(f\"spark.sql.catalog.{BIGLAKE_CATALOG}.warehouse\", f\"gs://{BUCKET_NAME}/{BIGLAKE_CATALOG}\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .config(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", GCP_KEY_PATH) \\\n",
    "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"‚úÖ Spark Session with BigLake catalog created successfully!\")\n",
    "print(f\"üîó Spark UI: http://localhost:4040\")\n",
    "\n",
    "# Create sample data for testing\n",
    "sample_data = [\n",
    "    (\"Alice\", 25, \"Engineering\", 75000),\n",
    "    (\"Bob\", 30, \"Marketing\", 65000),\n",
    "    (\"Charlie\", 35, \"Engineering\", 85000),\n",
    "    (\"Diana\", 28, \"Sales\", 60000),\n",
    "    (\"Eve\", 32, \"Engineering\", 90000)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(\n",
    "    sample_data, [\"name\", \"age\", \"department\", \"salary\"])\n",
    "\n",
    "print(\"üìä Sample DataFrame created:\")\n",
    "df.show()\n",
    "\n",
    "print(f\"‚úÖ Ready to work with BigLake Iceberg tables!\")\n",
    "print(f\"üìã Catalog: {BIGLAKE_CATALOG}\")\n",
    "print(f\"üóÑÔ∏è Dataset: {BIGLAKE_DATASET}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec8c191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CELL 4: BigLake Iceberg Operations\n",
    "# =============================================================================\n",
    "# Create and work with Iceberg tables in BigLake metastore\n",
    "\n",
    "# Table configuration\n",
    "TABLE_NAME = f\"{BIGLAKE_CATALOG}.{BIGLAKE_DATASET}.employees\"\n",
    "\n",
    "print(\"üèîÔ∏è Working with BigLake Iceberg tables...\")\n",
    "\n",
    "# Step 1: Create Iceberg table in BigLake metastore\n",
    "print(\"üìù Creating Iceberg table in BigLake metastore...\")\n",
    "\n",
    "try:\n",
    "    # Drop table if exists (for demo purposes)\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {TABLE_NAME}\")\n",
    "\n",
    "    # Create Iceberg table\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE TABLE {TABLE_NAME} (\n",
    "        name STRING,\n",
    "        age INT,\n",
    "        department STRING,\n",
    "        salary BIGINT\n",
    "    )\n",
    "    USING ICEBERG\n",
    "    TBLPROPERTIES (\n",
    "        'bq_connection'='projects/{PROJECT_ID}/locations/{REGION}/connections/default-{REGION}'\n",
    "    )\n",
    "    \"\"\")\n",
    "\n",
    "    print(f\"‚úÖ Iceberg table created: {TABLE_NAME}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Table creation failed: {e}\")\n",
    "    print(\"üí° Make sure BigQuery dataset and connection exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102448ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Insert sample data\n",
    "print(\"üì• Inserting sample data...\")\n",
    "\n",
    "try:\n",
    "    df.write \\\n",
    "        .format(\"iceberg\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(TABLE_NAME)\n",
    "\n",
    "    print(\"‚úÖ Data inserted successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Data insertion failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1991809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Query the table\n",
    "print(\"üîç Querying the Iceberg table...\")\n",
    "\n",
    "try:\n",
    "    result = spark.sql(f\"SELECT * FROM {TABLE_NAME}\")\n",
    "    print(\"--- All Records ---\")\n",
    "    result.show()\n",
    "\n",
    "    # Example queries\n",
    "    print(\"--- Engineering Department ---\")\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT name, age, salary\n",
    "        FROM {TABLE_NAME}\n",
    "        WHERE department = 'Engineering'\n",
    "        ORDER BY salary DESC\n",
    "    \"\"\").show()\n",
    "\n",
    "    print(\"--- Department Summary ---\")\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT department,\n",
    "               COUNT(*) as employee_count,\n",
    "               AVG(salary) as avg_salary,\n",
    "               MAX(salary) as max_salary\n",
    "        FROM {TABLE_NAME}\n",
    "        GROUP BY department\n",
    "        ORDER BY avg_salary DESC\n",
    "    \"\"\").show()\n",
    "\n",
    "    print(\"‚úÖ Queries executed successfully\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Query failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8733a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Show table metadata\n",
    "print(\"üìã Table Information:\")\n",
    "\n",
    "try:\n",
    "    # Show table schema\n",
    "    print(\"--- Table Schema ---\")\n",
    "    spark.sql(f\"DESCRIBE {TABLE_NAME}\").show()\n",
    "\n",
    "    # Show table properties\n",
    "    print(\"--- Table Properties ---\")\n",
    "    spark.sql(f\"SHOW TBLPROPERTIES {TABLE_NAME}\").show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Metadata query failed: {e}\")\n",
    "\n",
    "print(\"üéâ BigLake Iceberg demo completed!\")\n",
    "print(f\"üìä Table: {TABLE_NAME}\")\n",
    "print(f\"üîó You can now use this table for your data operations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913f2966",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1441f5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CLEANUP (Optional)\n",
    "# =============================================================================\n",
    "# Uncomment to stop Spark session and clean up\n",
    "\n",
    "# print(\"üßπ Cleaning up...\")\n",
    "# spark.stop()\n",
    "#\n",
    "# # To stop Docker containers:\n",
    "# # !docker-compose down\n",
    "#\n",
    "# # To remove all data:\n",
    "# # !docker-compose down -v"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
